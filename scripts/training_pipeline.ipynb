{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e37261",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üèÜ FINAL GOLD PIPELINE: Weighted 2-Model Ensemble\n",
    "-------------------------------------------------\n",
    "Architecture: Inception-SE + sEMG-Net (The Winner: 85.11%)\n",
    "Strategy:     Class Weights to fix the remaining Class 1 vs 2 confusion.\n",
    "Goal:         Break 86% Accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, mixed_precision, callbacks\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import mode\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "DATA_DIR = 'data'\n",
    "ARTIFACTS_DIR = 'artifacts_final'\n",
    "FS = 512\n",
    "EPOCHS = 60  # Increased slightly to allow class weights to settle\n",
    "BATCH_SIZE = 128\n",
    "RANDOM_SEED = 42\n",
    "VAL_FILE_RATIO = 0.50\n",
    "WINDOW_MS = 400\n",
    "STRIDE_MS = 160\n",
    "L2_REG = 1e-4\n",
    "\n",
    "# GPU Setup\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print(\"‚úÖ Mixed Precision (FP16) Enabled\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# ==================== DATA & PREPROCESSING ====================\n",
    "# (Standard Preprocessing Pipeline)\n",
    "\n",
    "class SignalPreprocessor:\n",
    "    def __init__(self, fs=1000, bandpass_low=20.0, bandpass_high=450.0, notch_freq=50.0):\n",
    "        self.fs = fs\n",
    "        nyq = fs / 2\n",
    "        low = max(0.001, min(bandpass_low / nyq, 0.99))\n",
    "        high = max(low + 0.01, min(bandpass_high / nyq, 0.999))\n",
    "        self.b_bp, self.a_bp = butter(4, [low, high], btype='band')\n",
    "        self.b_notch, self.a_notch = iirnotch(notch_freq, 30.0, self.fs) if notch_freq > 0 else (None, None)\n",
    "        self.channel_means, self.channel_stds = None, None\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, signals_list):\n",
    "        all_signals = np.concatenate(signals_list, axis=0)\n",
    "        self.channel_means = np.mean(all_signals, axis=0)\n",
    "        self.channel_stds = np.std(all_signals, axis=0) + 1e-8\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, signal):\n",
    "        if len(signal) > 12:\n",
    "            signal = filtfilt(self.b_bp, self.a_bp, signal, axis=0)\n",
    "            if self.b_notch is not None:\n",
    "                signal = filtfilt(self.b_notch, self.a_notch, signal, axis=0)\n",
    "        if self.fitted:\n",
    "            return (signal - self.channel_means) / self.channel_stds\n",
    "        return (signal - np.mean(signal, axis=0)) / (np.std(signal, axis=0) + 1e-8)\n",
    "\n",
    "    def segment(self, signal, window_ms=200, stride_ms=100):\n",
    "        win_sz = int(window_ms * self.fs / 1000)\n",
    "        step = int(stride_ms * self.fs / 1000)\n",
    "        n = len(signal)\n",
    "        if n < win_sz: return None\n",
    "        n_win = (n - win_sz) // step + 1\n",
    "        idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
    "        return signal[idx]\n",
    "\n",
    "def augment_dataset_advanced(X, y):\n",
    "    print(f\"    ‚ö° Augmenting Data (Input: {len(X)} windows)...\")\n",
    "    b, t, c = X.shape\n",
    "\n",
    "    # 1. Channel Masking\n",
    "    X_mask = X.copy()\n",
    "    mask_indices = np.random.choice(b, size=int(b * 0.5), replace=False)\n",
    "    for i in mask_indices:\n",
    "        ch = np.random.randint(0, c)\n",
    "        X_mask[i, :, ch] = 0\n",
    "    X_mask = X_mask + np.random.normal(0, 0.02, size=X_mask.shape)\n",
    "\n",
    "    # 2. MixUp\n",
    "    indices = np.random.permutation(b)\n",
    "    X_shuffled = X[indices]\n",
    "    alpha = 0.2\n",
    "    lam = np.random.beta(alpha, alpha, size=(b, 1, 1))\n",
    "    X_mix = lam * X + (1 - lam) * X_shuffled\n",
    "    y_mix = y.copy()\n",
    "\n",
    "    X_final = np.concatenate([X, X_mask, X_mix], axis=0)\n",
    "    y_final = np.concatenate([y, y, y_mix], axis=0)\n",
    "    print(f\"    ‚ö° Augmentation complete. Size: {len(X_final)} (3x)\")\n",
    "    return X_final, y_final\n",
    "\n",
    "def get_session_files(data_dir, sessions):\n",
    "    files = []\n",
    "    for session in sessions:\n",
    "        pattern = f'{data_dir}/**/{session}/**/*.csv'\n",
    "        files.extend(sorted(glob.glob(pattern, recursive=True)))\n",
    "    return files\n",
    "\n",
    "def split_files_by_ratio(files, val_ratio, seed=RANDOM_SEED):\n",
    "    gesture_files = {}\n",
    "    for f in files:\n",
    "        match = re.search(r'gesture(\\d+)', f)\n",
    "        if match:\n",
    "            g = int(match.group(1))\n",
    "            gesture_files.setdefault(g, []).append(f)\n",
    "    train, val = [], []\n",
    "    rng = random.Random(seed)\n",
    "    for g, gfiles in gesture_files.items():\n",
    "        rng.shuffle(gfiles)\n",
    "        n_val = max(1, int(len(gfiles) * val_ratio))\n",
    "        val.extend(gfiles[:n_val])\n",
    "        train.extend(gfiles[n_val:])\n",
    "    return train, val\n",
    "\n",
    "def load_files_data(file_list):\n",
    "    data_list, labels_list = [], []\n",
    "    for f in file_list:\n",
    "        try:\n",
    "            lbl = int(re.search(r'gesture(\\d+)', f).group(1))\n",
    "            d = pd.read_csv(f).values\n",
    "            if d.shape[1] >= 8:\n",
    "                data_list.append(d)\n",
    "                labels_list.append(np.full(len(d), lbl))\n",
    "        except: pass\n",
    "    return data_list, labels_list\n",
    "\n",
    "def window_data(data_list, labels_list, prep, window_ms, stride_ms):\n",
    "    X_wins, y_wins = [], []\n",
    "    win_sz = int(window_ms * FS / 1000)\n",
    "    step = int(stride_ms * FS / 1000)\n",
    "    for d, l in zip(data_list, labels_list):\n",
    "        d_filt = prep.transform(d)\n",
    "        w = prep.segment(d_filt, window_ms, stride_ms)\n",
    "        if w is not None:\n",
    "            X_wins.append(w)\n",
    "            n_win = (len(d) - win_sz) // step + 1\n",
    "            idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
    "            w_modes = mode(l[idx], axis=1, keepdims=True)[0].flatten()\n",
    "            y_wins.append(w_modes)\n",
    "    if not X_wins: return None, None\n",
    "    return np.concatenate(X_wins), np.concatenate(y_wins)\n",
    "\n",
    "# ==================== MODEL DEFINITIONS ====================\n",
    "\n",
    "# 1. Inception-SE-TCN\n",
    "def squeeze_excite_block(input_tensor, ratio=8):\n",
    "    filters = input_tensor.shape[-1]\n",
    "    se = layers.GlobalAveragePooling1D()(input_tensor)\n",
    "    se = layers.Dense(filters // ratio, activation='relu', kernel_regularizer=l2(L2_REG))(se)\n",
    "    se = layers.Dense(filters, activation='sigmoid', kernel_regularizer=l2(L2_REG))(se)\n",
    "    se = layers.Reshape((1, filters))(se)\n",
    "    return layers.Multiply()([input_tensor, se])\n",
    "\n",
    "def inception_block(x, filters, dilation_rate):\n",
    "    b1 = layers.Conv1D(filters//2, 3, dilation_rate=dilation_rate, padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
    "    b1 = layers.BatchNormalization()(b1)\n",
    "    b1 = layers.Activation('relu')(b1)\n",
    "    b2 = layers.Conv1D(filters//2, 7, dilation_rate=dilation_rate, padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
    "    b2 = layers.BatchNormalization()(b2)\n",
    "    b2 = layers.Activation('relu')(b2)\n",
    "    return layers.Concatenate()([b1, b2])\n",
    "\n",
    "def make_inception_se_tcn(input_shape, n_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.GaussianNoise(0.05)(inputs)\n",
    "    filters = 64\n",
    "    for dilation_rate in [1, 2, 4, 8]:\n",
    "        prev_x = x\n",
    "        x = inception_block(x, filters, dilation_rate)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = squeeze_excite_block(x, ratio=8)\n",
    "        if prev_x.shape[-1] != filters:\n",
    "            prev_x = layers.Conv1D(filters=filters, kernel_size=1, padding='same')(prev_x)\n",
    "        x = layers.Add()([x, prev_x])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.MultiHeadAttention(key_dim=64, num_heads=4, dropout=0.3)(x, x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
    "    return keras.Model(inputs, outputs, name='Inception_SE_Attn')\n",
    "\n",
    "# 2. sEMG Net\n",
    "def conv_block(x, filters, kernel_size, pool=True):\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    if pool: x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    return x\n",
    "\n",
    "def make_semg_net(input_shape, n_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.GaussianNoise(0.05)(inputs)\n",
    "    x = conv_block(x, 64, 9, pool=False)\n",
    "    x = conv_block(x, 128, 5, pool=True)\n",
    "    x = conv_block(x, 256, 3, pool=True)\n",
    "    x = conv_block(x, 512, 3, pool=True)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
    "    return keras.Model(inputs, outputs, name='sEMG_Net')\n",
    "\n",
    "# ==================== 4. TRAINING LOGIC ====================\n",
    "\n",
    "def train_and_save(model_builder, model_name, X_train, y_train_hot, X_val, y_val_hot, input_shape, n_classes, class_weights):\n",
    "    print(f\"\\nüèãÔ∏è TRAIN: {model_name} (Weighted)\")\n",
    "\n",
    "    total_steps = len(X_train) // BATCH_SIZE * EPOCHS\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate=0.001, first_decay_steps=int(total_steps * 0.3),\n",
    "        t_mul=2.0, m_mul=0.9, alpha=1e-5\n",
    "    )\n",
    "\n",
    "    save_path = f'{ARTIFACTS_DIR}/best_{model_name}.keras'\n",
    "    checkpoint = callbacks.ModelCheckpoint(save_path, monitor='val_accuracy', mode='max', save_best_only=True, verbose=0)\n",
    "\n",
    "    model = model_builder(input_shape, n_classes)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # üî• APPLY CLASS WEIGHTS HERE\n",
    "    model.fit(X_train, y_train_hot,\n",
    "              validation_data=(X_val, y_val_hot),\n",
    "              epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "              callbacks=[checkpoint],\n",
    "              class_weight=class_weights,  # <--- The Key Fix\n",
    "              verbose=1)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    return save_path\n",
    "\n",
    "def main():\n",
    "    os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "    print(\"=\"*70 + \"\\nüöÄ FINAL GOLD PIPELINE: 2-Model Ensemble + Class Weights\\n\" + \"=\"*70)\n",
    "\n",
    "    # 1. Load Data\n",
    "    existing_csvs = glob.glob(f'{DATA_DIR}/**/*.csv', recursive=True)\n",
    "    if not existing_csvs:\n",
    "        import gdown, zipfile\n",
    "        gdown.download('https://drive.google.com/uc?id=16iNEwhThf2LcX7rOOVM03MTZiwq7G51x', 'dataset.zip', quiet=False)\n",
    "        with zipfile.ZipFile('dataset.zip', 'r') as z: z.extractall(DATA_DIR)\n",
    "        os.remove('dataset.zip')\n",
    "\n",
    "    train_files = get_session_files(DATA_DIR, ['Session1', 'Session2'])\n",
    "    session3_files = get_session_files(DATA_DIR, ['Session3'])\n",
    "    val_files, test_files = split_files_by_ratio(session3_files, VAL_FILE_RATIO)\n",
    "\n",
    "    train_data, train_labels = load_files_data(train_files)\n",
    "    val_data, val_labels = load_files_data(val_files)\n",
    "    test_data, test_labels = load_files_data(test_files)\n",
    "\n",
    "    # 2. Preprocess\n",
    "    prep = SignalPreprocessor(fs=FS).fit(train_data)\n",
    "    X_train, y_train_raw = window_data(train_data, train_labels, prep, WINDOW_MS, STRIDE_MS)\n",
    "    X_val, y_val_raw = window_data(val_data, val_labels, prep, WINDOW_MS, STRIDE_MS)\n",
    "    X_test, y_test_raw = window_data(test_data, test_labels, prep, WINDOW_MS, STRIDE_MS)\n",
    "\n",
    "    # 3. Augment\n",
    "    X_train, y_train_raw = augment_dataset_advanced(X_train, y_train_raw)\n",
    "\n",
    "    # 4. Encode\n",
    "    le = LabelEncoder().fit(y_train_raw)\n",
    "    y_train = le.transform(y_train_raw)\n",
    "    y_val = np.array([le.transform([l])[0] if l in le.classes_ else -1 for l in y_val_raw])\n",
    "    y_test = np.array([le.transform([l])[0] if l in le.classes_ else -1 for l in y_test_raw])\n",
    "\n",
    "    n_classes = len(le.classes_)\n",
    "    input_shape = X_train.shape[1:]\n",
    "\n",
    "    y_train_hot = tf.keras.utils.to_categorical(y_train, n_classes)\n",
    "    y_val_hot = tf.keras.utils.to_categorical(y_val, n_classes)\n",
    "\n",
    "    # ==================== 5. DEFINE CLASS WEIGHTS ====================\n",
    "    # Based on your confusion matrix: Class 1 and 2 are the weak points.\n",
    "    class_weights = {\n",
    "        0: 1.0,\n",
    "        1: 1.5,  # Focus 50% more on Class 1\n",
    "        2: 1.5,  # Focus 50% more on Class 2\n",
    "        3: 1.0,\n",
    "        4: 1.0\n",
    "    }\n",
    "    print(f\"üéØ Strategy: Applying Class Weights: {class_weights}\")\n",
    "\n",
    "    # ==================== 6. TRAIN & SAVE ====================\n",
    "    path_inception = train_and_save(make_inception_se_tcn, \"inception_se\",\n",
    "                                    X_train, y_train_hot, X_val, y_val_hot, input_shape, n_classes, class_weights)\n",
    "\n",
    "    path_semgnet = train_and_save(make_semg_net, \"semg_net\",\n",
    "                                  X_train, y_train_hot, X_val, y_val_hot, input_shape, n_classes, class_weights)\n",
    "\n",
    "    # ==================== 7. FINAL EVALUATION ====================\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\nüéØ FINAL EVALUATION (2-Model Weighted)\\n\" + \"=\"*70)\n",
    "\n",
    "    m1 = make_inception_se_tcn(input_shape, n_classes)\n",
    "    m1.load_weights(path_inception)\n",
    "\n",
    "    m2 = make_semg_net(input_shape, n_classes)\n",
    "    m2.load_weights(path_semgnet)\n",
    "\n",
    "    p1 = m1.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "    p2 = m2.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "    # Simple Average (since models are equally strong)\n",
    "    ensemble_probs = (p1 + p2) / 2.0\n",
    "    ensemble_preds = ensemble_probs.argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, ensemble_preds)\n",
    "    f1 = f1_score(y_test, ensemble_preds, average='macro')\n",
    "\n",
    "    print(f\"üèÜ FINAL ACCURACY: {acc:.4f}\")\n",
    "    print(f\"üèÜ FINAL F1 SCORE: {f1:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

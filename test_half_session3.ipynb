{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41573b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üöÄ FINAL TEST SCRIPT: 2-MODEL ENSEMBLE\n",
    "--------------------------------------\n",
    "Evaluates the Inception-SE + sEMG-Net ensemble.\n",
    "\n",
    "LOGIC:\n",
    "1. Re-creates the specific test split from Session 3.\n",
    "2. Fits the preprocessor on training data (to match training scaling).\n",
    "3. Loads the best weights for both models.\n",
    "4. Performs Soft Voting (50/50 split).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from scipy.stats import mode\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "DATA_DIR = 'data'\n",
    "ARTIFACTS_DIR = 'artifacts_final'\n",
    "FS = 512\n",
    "WINDOW_MS = 400\n",
    "STRIDE_MS = 160\n",
    "BATCH_SIZE = 128\n",
    "L2_REG = 1e-4\n",
    "\n",
    "# ==================== ARCHITECTURES (Must Match Training) ====================\n",
    "\n",
    "def squeeze_excite_block(input_tensor, ratio=8):\n",
    "    filters = input_tensor.shape[-1]\n",
    "    se = layers.GlobalAveragePooling1D()(input_tensor)\n",
    "    se = layers.Dense(filters // ratio, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(se)\n",
    "    se = layers.Dense(filters, activation='sigmoid', kernel_regularizer=regularizers.l2(L2_REG))(se)\n",
    "    se = layers.Reshape((1, filters))(se)\n",
    "    return layers.Multiply()([input_tensor, se])\n",
    "\n",
    "def inception_block(x, filters, dilation_rate):\n",
    "    b1 = layers.Conv1D(filters=filters//2, kernel_size=3, dilation_rate=dilation_rate,\n",
    "                       padding='same', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "    b1 = layers.BatchNormalization()(b1)\n",
    "    b1 = layers.Activation('relu')(b1)\n",
    "    b2 = layers.Conv1D(filters=filters//2, kernel_size=7, dilation_rate=dilation_rate,\n",
    "                       padding='same', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "    b2 = layers.BatchNormalization()(b2)\n",
    "    b2 = layers.Activation('relu')(b2)\n",
    "    return layers.Concatenate()([b1, b2])\n",
    "\n",
    "def make_inception_se_tcn(input_shape, n_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.GaussianNoise(0.05)(inputs) # Noise layer is active only in training, safe to leave here\n",
    "    filters = 64\n",
    "    for dilation_rate in [1, 2, 4, 8]:\n",
    "        prev_x = x\n",
    "        x = inception_block(x, filters, dilation_rate)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = squeeze_excite_block(x, ratio=8)\n",
    "        if prev_x.shape[-1] != filters:\n",
    "            prev_x = layers.Conv1D(filters=filters, kernel_size=1, padding='same')(prev_x)\n",
    "        x = layers.Add()([x, prev_x])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.MultiHeadAttention(key_dim=64, num_heads=4, dropout=0.3)(x, x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
    "    return keras.Model(inputs, outputs, name='Inception_SE_Attn')\n",
    "\n",
    "def conv_block(x, filters, kernel_size, pool=True):\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    if pool: x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    return x\n",
    "\n",
    "def make_semg_net(input_shape, n_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.GaussianNoise(0.05)(inputs)\n",
    "    x = conv_block(x, 64, 9, pool=False)\n",
    "    x = conv_block(x, 128, 5, pool=True)\n",
    "    x = conv_block(x, 256, 3, pool=True)\n",
    "    x = conv_block(x, 512, 3, pool=True)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
    "    return keras.Model(inputs, outputs, name='sEMG_Net')\n",
    "\n",
    "# ==================== DATA UTILS ====================\n",
    "\n",
    "class SignalPreprocessor:\n",
    "    def __init__(self, fs=1000, bandpass_low=20.0, bandpass_high=450.0, notch_freq=50.0):\n",
    "        self.fs = fs\n",
    "        self.nyq = fs / 2\n",
    "        low = max(0.001, min(bandpass_low / self.nyq, 0.99))\n",
    "        high = max(low + 0.01, min(bandpass_high / self.nyq, 0.999))\n",
    "        self.b_bp, self.a_bp = butter(4, [low, high], btype='band')\n",
    "        self.b_notch, self.a_notch = iirnotch(notch_freq, 30.0, self.fs) if notch_freq > 0 else (None, None)\n",
    "        self.channel_means, self.channel_stds = None, None\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, signals_list):\n",
    "        all_signals = np.concatenate(signals_list, axis=0)\n",
    "        self.channel_means = np.mean(all_signals, axis=0)\n",
    "        self.channel_stds = np.std(all_signals, axis=0) + 1e-8\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, signal):\n",
    "        if len(signal) > 12:\n",
    "            signal = filtfilt(self.b_bp, self.a_bp, signal, axis=0)\n",
    "            if self.b_notch is not None:\n",
    "                signal = filtfilt(self.b_notch, self.a_notch, signal, axis=0)\n",
    "        if self.fitted:\n",
    "            return (signal - self.channel_means) / self.channel_stds\n",
    "        return (signal - np.mean(signal, axis=0)) / (np.std(signal, axis=0) + 1e-8)\n",
    "\n",
    "    def segment(self, signal, window_ms=200, stride_ms=100):\n",
    "        win_sz = int(window_ms * self.fs / 1000)\n",
    "        step = int(stride_ms * self.fs / 1000)\n",
    "        n = len(signal)\n",
    "        if n < win_sz: return None\n",
    "        n_win = (n - win_sz) // step + 1\n",
    "        idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
    "        return signal[idx]\n",
    "\n",
    "def get_session_files(data_dir, sessions):\n",
    "    files = []\n",
    "    for session in sessions:\n",
    "        pattern = f'{data_dir}/**/{session}/**/*.csv'\n",
    "        files.extend(sorted(glob.glob(pattern, recursive=True)))\n",
    "    return files\n",
    "\n",
    "def load_files_data(file_list):\n",
    "    data_list, labels_list = [], []\n",
    "    for f in file_list:\n",
    "        try:\n",
    "            lbl = int(re.search(r'gesture(\\d+)', f).group(1))\n",
    "            d = pd.read_csv(f).values\n",
    "            if d.shape[1] >= 8:\n",
    "                data_list.append(d)\n",
    "                labels_list.append(np.full(len(d), lbl))\n",
    "        except: pass\n",
    "    return data_list, labels_list\n",
    "\n",
    "def window_data(data_list, labels_list, prep, window_ms, stride_ms):\n",
    "    X_wins, y_wins = [], []\n",
    "    win_sz = int(window_ms * FS / 1000)\n",
    "    step = int(stride_ms * FS / 1000)\n",
    "    for d, l in zip(data_list, labels_list):\n",
    "        d_filt = prep.transform(d)\n",
    "        w = prep.segment(d_filt, window_ms, stride_ms)\n",
    "        if w is not None:\n",
    "            X_wins.append(w)\n",
    "            n_win = (len(d) - win_sz) // step + 1\n",
    "            idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
    "            w_modes = mode(l[idx], axis=1, keepdims=True)[0].flatten()\n",
    "            y_wins.append(w_modes)\n",
    "    if not X_wins: return None, None\n",
    "    return np.concatenate(X_wins), np.concatenate(y_wins)\n",
    "\n",
    "# ==================== MAIN ====================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"üöÄ TESTING 2-MODEL ENSEMBLE (Inception-SE + sEMG-Net)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. SETUP DATA (Split Reproduction)\n",
    "    print(\"\\nüì¶ Loading Dataset configuration...\")\n",
    "    train_files = get_session_files(DATA_DIR, ['Session1', 'Session2'])\n",
    "    session3_files = get_session_files(DATA_DIR, ['Session3'])\n",
    "\n",
    "    # Reproduce the random split used in training\n",
    "    gesture_files = {}\n",
    "    for f in session3_files:\n",
    "        match = re.search(r'gesture(\\d+)', f)\n",
    "        if match:\n",
    "            g = int(match.group(1))\n",
    "            gesture_files.setdefault(g, []).append(f)\n",
    "\n",
    "    test_files_final = []\n",
    "    rng = random.Random(42) # Match seed from training\n",
    "\n",
    "    for g, gfiles in gesture_files.items():\n",
    "        rng.shuffle(gfiles)\n",
    "        # Training logic: val_files = [:n_val], test_files = [n_val:]\n",
    "        n_val = max(1, int(len(gfiles) * 0.50))\n",
    "        test_files_final.extend(gfiles[n_val:]) # This corresponds to the 'test' set in your training script\n",
    "\n",
    "    print(f\"üìÑ Found {len(test_files_final)} test files.\")\n",
    "\n",
    "    # 2. LOAD & PREPROCESS\n",
    "    print(\"‚è≥ Loading raw data...\")\n",
    "    # We load training data ONLY to fit the Scaler (Preprocssor)\n",
    "    train_data_raw, train_labels_raw = load_files_data(train_files)\n",
    "    test_data_raw, test_labels_raw = load_files_data(test_files_final)\n",
    "\n",
    "    print(\"üîß Preprocessing...\")\n",
    "    prep = SignalPreprocessor(fs=FS)\n",
    "    prep.fit(train_data_raw)\n",
    "\n",
    "    X_test, y_test_raw = window_data(test_data_raw, test_labels_raw, prep, WINDOW_MS, STRIDE_MS)\n",
    "\n",
    "    # 3. LABEL ENCODING\n",
    "    _, y_train_raw_dummy = window_data(train_data_raw, train_labels_raw, prep, WINDOW_MS, STRIDE_MS)\n",
    "    le = LabelEncoder().fit(y_train_raw_dummy)\n",
    "    y_test = np.array([le.transform([l])[0] if l in le.classes_ else -1 for l in y_test_raw])\n",
    "    n_classes = len(le.classes_)\n",
    "    input_shape = X_test.shape[1:]\n",
    "\n",
    "    print(f\"‚úÖ Data Ready. Shape: {X_test.shape}\")\n",
    "\n",
    "    # 4. LOAD MODELS\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\nüîÆ RUNNING ENSEMBLE INFERENCE\\n\" + \"-\"*40)\n",
    "\n",
    "    models_config = [\n",
    "        (\"inception_se\", make_inception_se_tcn),\n",
    "        (\"semg_net\", make_semg_net)\n",
    "    ]\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for name, builder in models_config:\n",
    "        path = f'{ARTIFACTS_DIR}/best_{name}.keras'\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"‚ùå Error: Could not find {path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"‚ö° Loading {name}...\")\n",
    "        model = builder(input_shape, n_classes)\n",
    "        model.load_weights(path)\n",
    "\n",
    "        probs = model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "        predictions.append(probs)\n",
    "\n",
    "        # Individual Check\n",
    "        acc = accuracy_score(y_test, probs.argmax(axis=1))\n",
    "        print(f\"   -> {name} Standalone Acc: {acc:.4f}\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    # 5. SOFT VOTING (50/50 Split)\n",
    "    if len(predictions) == 2:\n",
    "        ensemble_probs = (predictions[0] + predictions[1]) / 2.0\n",
    "        final_preds = ensemble_probs.argmax(axis=1)\n",
    "\n",
    "        # 6. RESULTS\n",
    "        acc = accuracy_score(y_test, final_preds)\n",
    "        f1 = f1_score(y_test, final_preds, average='macro')\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üèÜ FINAL ENSEMBLE RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ACCURACY: {acc:.4f}\")\n",
    "        print(f\"F1 SCORE: {f1:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"\\nClassification Report:\\n\")\n",
    "        print(classification_report(y_test, final_preds, target_names=[str(c) for c in le.classes_]))\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, final_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
    "                    xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "        plt.title(f'2-Model Ensemble Matrix (Acc: {acc:.4f})')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig(f'{ARTIFACTS_DIR}/ensemble_2model_matrix.png')\n",
    "        print(f\"\\nüìä Matrix saved to {ARTIFACTS_DIR}/ensemble_2model_matrix.png\")\n",
    "    else:\n",
    "        print(\"‚ùå Error: Not enough models loaded for ensemble.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

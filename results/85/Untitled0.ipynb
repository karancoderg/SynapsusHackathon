{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_Iz6IQJawQtp",
        "outputId": "e0d99b01-1790-48a2-ca55-28b700034fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Mixed Precision (FP16) Enabled\n",
            "======================================================================\n",
            "ğŸš€ TRAIN: sEMG NET (Deep CNN)\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16iNEwhThf2LcX7rOOVM03MTZiwq7G51x\n",
            "From (redirected): https://drive.google.com/uc?id=16iNEwhThf2LcX7rOOVM03MTZiwq7G51x&confirm=t&uuid=685160e5-4d2a-472f-8105-65467d7aefae\n",
            "To: /content/dataset.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 489M/489M [00:05<00:00, 84.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Data Loaded] Train: 1750 | Val: 440 | Test: 435\n",
            "\n",
            "[Windowing] Processing...\n",
            "   âš¡ Augmenting Data (Input: 52500 windows)...\n",
            "   âš¡ Augmentation complete. Size: 157500 (3x)\n",
            "\n",
            "----------------------------------------\n",
            "ğŸ‹ï¸ TRAIN MODEL: sEMG NET\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sEMG_Net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sEMG_Net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m8\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gaussian_noise (\u001b[38;5;33mGaussianNoise\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m8\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚         \u001b[38;5;34m4,672\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ activation (\u001b[38;5;33mActivation\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m41,088\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m204\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m102\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m102\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚        \u001b[38;5;34m98,560\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m102\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m102\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m102\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m, \u001b[38;5;34m256\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚       \u001b[38;5;34m393,728\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_3           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚         \u001b[38;5;34m2,048\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling1d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚       \u001b[38;5;34m262,656\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              â”‚         \u001b[38;5;34m2,565\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gaussian_noise (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianNoise</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,672</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">204</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_3           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling1d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m807,109\u001b[0m (3.08 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">807,109</span> (3.08 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m805,189\u001b[0m (3.07 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">805,189</span> (3.07 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,920\u001b[0m (7.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> (7.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5492 - loss: 1.3529\n",
            "Epoch 1: val_accuracy improved from -inf to 0.74386, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 20ms/step - accuracy: 0.5493 - loss: 1.3529 - val_accuracy: 0.7439 - val_loss: 0.9261 - learning_rate: 0.0010\n",
            "Epoch 2/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6834 - loss: 1.1278\n",
            "Epoch 2: val_accuracy did not improve from 0.74386\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.6834 - loss: 1.1278 - val_accuracy: 0.7286 - val_loss: 0.9441 - learning_rate: 0.0010\n",
            "Epoch 3/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7036 - loss: 1.0865\n",
            "Epoch 3: val_accuracy improved from 0.74386 to 0.76470, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7036 - loss: 1.0864 - val_accuracy: 0.7647 - val_loss: 0.8796 - learning_rate: 0.0010\n",
            "Epoch 4/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7153 - loss: 1.0686\n",
            "Epoch 4: val_accuracy did not improve from 0.76470\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.7153 - loss: 1.0686 - val_accuracy: 0.7427 - val_loss: 0.9141 - learning_rate: 0.0010\n",
            "Epoch 5/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7243 - loss: 1.0583\n",
            "Epoch 5: val_accuracy improved from 0.76470 to 0.76689, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7243 - loss: 1.0583 - val_accuracy: 0.7669 - val_loss: 0.8821 - learning_rate: 0.0010\n",
            "Epoch 6/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7335 - loss: 1.0446\n",
            "Epoch 6: val_accuracy improved from 0.76689 to 0.77273, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7335 - loss: 1.0446 - val_accuracy: 0.7727 - val_loss: 0.8653 - learning_rate: 0.0010\n",
            "Epoch 7/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7384 - loss: 1.0367\n",
            "Epoch 7: val_accuracy did not improve from 0.77273\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7384 - loss: 1.0367 - val_accuracy: 0.7575 - val_loss: 0.8976 - learning_rate: 0.0010\n",
            "Epoch 8/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7464 - loss: 1.0265\n",
            "Epoch 8: val_accuracy did not improve from 0.77273\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7464 - loss: 1.0265 - val_accuracy: 0.7660 - val_loss: 0.9192 - learning_rate: 0.0010\n",
            "Epoch 9/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7506 - loss: 1.0203\n",
            "Epoch 9: val_accuracy did not improve from 0.77273\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7506 - loss: 1.0203 - val_accuracy: 0.7676 - val_loss: 0.9108 - learning_rate: 0.0010\n",
            "Epoch 10/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7547 - loss: 1.0168\n",
            "Epoch 10: val_accuracy improved from 0.77273 to 0.78470, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7547 - loss: 1.0168 - val_accuracy: 0.7847 - val_loss: 0.8956 - learning_rate: 0.0010\n",
            "Epoch 11/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7588 - loss: 1.0089\n",
            "Epoch 11: val_accuracy did not improve from 0.78470\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7588 - loss: 1.0089 - val_accuracy: 0.7712 - val_loss: 0.9023 - learning_rate: 0.0010\n",
            "Epoch 12/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7620 - loss: 1.0006\n",
            "Epoch 12: val_accuracy did not improve from 0.78470\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7620 - loss: 1.0006 - val_accuracy: 0.7752 - val_loss: 0.8930 - learning_rate: 0.0010\n",
            "Epoch 13/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7661 - loss: 0.9975\n",
            "Epoch 13: val_accuracy did not improve from 0.78470\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7661 - loss: 0.9975 - val_accuracy: 0.7677 - val_loss: 0.9146 - learning_rate: 0.0010\n",
            "Epoch 14/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7684 - loss: 0.9941\n",
            "Epoch 14: val_accuracy did not improve from 0.78470\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7684 - loss: 0.9941 - val_accuracy: 0.7768 - val_loss: 0.8802 - learning_rate: 0.0010\n",
            "Epoch 15/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7719 - loss: 0.9900\n",
            "Epoch 15: val_accuracy improved from 0.78470 to 0.78614, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7719 - loss: 0.9900 - val_accuracy: 0.7861 - val_loss: 0.8607 - learning_rate: 0.0010\n",
            "Epoch 16/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7740 - loss: 0.9869\n",
            "Epoch 16: val_accuracy did not improve from 0.78614\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7740 - loss: 0.9869 - val_accuracy: 0.7848 - val_loss: 0.8705 - learning_rate: 0.0010\n",
            "Epoch 17/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7744 - loss: 0.9866\n",
            "Epoch 17: val_accuracy improved from 0.78614 to 0.80144, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7744 - loss: 0.9866 - val_accuracy: 0.8014 - val_loss: 0.8557 - learning_rate: 0.0010\n",
            "Epoch 18/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7768 - loss: 0.9833\n",
            "Epoch 18: val_accuracy did not improve from 0.80144\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7768 - loss: 0.9833 - val_accuracy: 0.7863 - val_loss: 0.8565 - learning_rate: 0.0010\n",
            "Epoch 19/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7778 - loss: 0.9830\n",
            "Epoch 19: val_accuracy improved from 0.80144 to 0.80318, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7778 - loss: 0.9830 - val_accuracy: 0.8032 - val_loss: 0.8397 - learning_rate: 0.0010\n",
            "Epoch 20/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7802 - loss: 0.9781\n",
            "Epoch 20: val_accuracy did not improve from 0.80318\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7802 - loss: 0.9781 - val_accuracy: 0.7862 - val_loss: 0.8638 - learning_rate: 0.0010\n",
            "Epoch 21/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7812 - loss: 0.9776\n",
            "Epoch 21: val_accuracy did not improve from 0.80318\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7812 - loss: 0.9776 - val_accuracy: 0.7933 - val_loss: 0.8531 - learning_rate: 0.0010\n",
            "Epoch 22/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7831 - loss: 0.9751\n",
            "Epoch 22: val_accuracy did not improve from 0.80318\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7831 - loss: 0.9751 - val_accuracy: 0.7764 - val_loss: 0.8832 - learning_rate: 0.0010\n",
            "Epoch 23/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7840 - loss: 0.9727\n",
            "Epoch 23: val_accuracy did not improve from 0.80318\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7840 - loss: 0.9727 - val_accuracy: 0.7879 - val_loss: 0.8488 - learning_rate: 0.0010\n",
            "Epoch 24/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7851 - loss: 0.9716\n",
            "Epoch 24: val_accuracy did not improve from 0.80318\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7851 - loss: 0.9716 - val_accuracy: 0.8007 - val_loss: 0.8316 - learning_rate: 0.0010\n",
            "Epoch 25/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7857 - loss: 0.9710\n",
            "Epoch 25: val_accuracy did not improve from 0.80318\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7857 - loss: 0.9710 - val_accuracy: 0.7833 - val_loss: 0.8592 - learning_rate: 0.0010\n",
            "Epoch 26/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7865 - loss: 0.9708\n",
            "Epoch 26: val_accuracy did not improve from 0.80318\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7865 - loss: 0.9708 - val_accuracy: 0.7892 - val_loss: 0.8561 - learning_rate: 0.0010\n",
            "Epoch 27/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7970 - loss: 0.9535\n",
            "Epoch 27: val_accuracy improved from 0.80318 to 0.82015, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7970 - loss: 0.9535 - val_accuracy: 0.8202 - val_loss: 0.8021 - learning_rate: 5.0000e-04\n",
            "Epoch 28/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8002 - loss: 0.9433\n",
            "Epoch 28: val_accuracy did not improve from 0.82015\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8002 - loss: 0.9433 - val_accuracy: 0.8200 - val_loss: 0.8054 - learning_rate: 5.0000e-04\n",
            "Epoch 29/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8023 - loss: 0.9392\n",
            "Epoch 29: val_accuracy did not improve from 0.82015\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.8023 - loss: 0.9392 - val_accuracy: 0.8164 - val_loss: 0.8107 - learning_rate: 5.0000e-04\n",
            "Epoch 30/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8028 - loss: 0.9376\n",
            "Epoch 30: val_accuracy improved from 0.82015 to 0.82356, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8028 - loss: 0.9376 - val_accuracy: 0.8236 - val_loss: 0.7986 - learning_rate: 5.0000e-04\n",
            "Epoch 31/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8029 - loss: 0.9358\n",
            "Epoch 31: val_accuracy did not improve from 0.82356\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8029 - loss: 0.9358 - val_accuracy: 0.8162 - val_loss: 0.8055 - learning_rate: 5.0000e-04\n",
            "Epoch 32/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8058 - loss: 0.9319\n",
            "Epoch 32: val_accuracy did not improve from 0.82356\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8058 - loss: 0.9319 - val_accuracy: 0.8120 - val_loss: 0.8124 - learning_rate: 5.0000e-04\n",
            "Epoch 33/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8056 - loss: 0.9325\n",
            "Epoch 33: val_accuracy did not improve from 0.82356\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8056 - loss: 0.9325 - val_accuracy: 0.8165 - val_loss: 0.8056 - learning_rate: 5.0000e-04\n",
            "Epoch 34/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8057 - loss: 0.9303\n",
            "Epoch 34: val_accuracy did not improve from 0.82356\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8057 - loss: 0.9303 - val_accuracy: 0.8205 - val_loss: 0.8127 - learning_rate: 5.0000e-04\n",
            "Epoch 35/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8064 - loss: 0.9289\n",
            "Epoch 35: val_accuracy did not improve from 0.82356\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8064 - loss: 0.9289 - val_accuracy: 0.8118 - val_loss: 0.8074 - learning_rate: 5.0000e-04\n",
            "Epoch 36/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8058 - loss: 0.9299\n",
            "Epoch 36: val_accuracy did not improve from 0.82356\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8058 - loss: 0.9299 - val_accuracy: 0.8036 - val_loss: 0.8378 - learning_rate: 5.0000e-04\n",
            "Epoch 37/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8066 - loss: 0.9277\n",
            "Epoch 37: val_accuracy improved from 0.82356 to 0.82977, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8066 - loss: 0.9277 - val_accuracy: 0.8298 - val_loss: 0.7957 - learning_rate: 5.0000e-04\n",
            "Epoch 38/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8076 - loss: 0.9259\n",
            "Epoch 38: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8076 - loss: 0.9260 - val_accuracy: 0.8159 - val_loss: 0.8111 - learning_rate: 5.0000e-04\n",
            "Epoch 39/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8069 - loss: 0.9261\n",
            "Epoch 39: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8069 - loss: 0.9261 - val_accuracy: 0.8156 - val_loss: 0.8102 - learning_rate: 5.0000e-04\n",
            "Epoch 40/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8082 - loss: 0.9250\n",
            "Epoch 40: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8082 - loss: 0.9250 - val_accuracy: 0.8026 - val_loss: 0.8364 - learning_rate: 5.0000e-04\n",
            "Epoch 41/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8079 - loss: 0.9255\n",
            "Epoch 41: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8079 - loss: 0.9255 - val_accuracy: 0.8199 - val_loss: 0.8032 - learning_rate: 5.0000e-04\n",
            "Epoch 42/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8088 - loss: 0.9237\n",
            "Epoch 42: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8088 - loss: 0.9238 - val_accuracy: 0.8103 - val_loss: 0.8219 - learning_rate: 5.0000e-04\n",
            "Epoch 43/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8093 - loss: 0.9236\n",
            "Epoch 43: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8093 - loss: 0.9236 - val_accuracy: 0.8070 - val_loss: 0.8307 - learning_rate: 5.0000e-04\n",
            "Epoch 44/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8091 - loss: 0.9238\n",
            "Epoch 44: val_accuracy did not improve from 0.82977\n",
            "\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8091 - loss: 0.9238 - val_accuracy: 0.8101 - val_loss: 0.8244 - learning_rate: 5.0000e-04\n",
            "Epoch 45/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8142 - loss: 0.9137\n",
            "Epoch 45: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.8142 - loss: 0.9137 - val_accuracy: 0.8259 - val_loss: 0.7983 - learning_rate: 2.5000e-04\n",
            "Epoch 46/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8185 - loss: 0.9066\n",
            "Epoch 46: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8185 - loss: 0.9066 - val_accuracy: 0.8219 - val_loss: 0.8009 - learning_rate: 2.5000e-04\n",
            "Epoch 47/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8182 - loss: 0.9048\n",
            "Epoch 47: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8182 - loss: 0.9048 - val_accuracy: 0.8267 - val_loss: 0.7854 - learning_rate: 2.5000e-04\n",
            "Epoch 48/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8184 - loss: 0.9031\n",
            "Epoch 48: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8184 - loss: 0.9032 - val_accuracy: 0.8155 - val_loss: 0.8033 - learning_rate: 2.5000e-04\n",
            "Epoch 49/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8190 - loss: 0.9018\n",
            "Epoch 49: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8190 - loss: 0.9018 - val_accuracy: 0.8273 - val_loss: 0.7907 - learning_rate: 2.5000e-04\n",
            "Epoch 50/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8193 - loss: 0.9011\n",
            "Epoch 50: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8193 - loss: 0.9011 - val_accuracy: 0.8282 - val_loss: 0.7859 - learning_rate: 2.5000e-04\n",
            "Epoch 51/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8194 - loss: 0.9004\n",
            "Epoch 51: val_accuracy did not improve from 0.82977\n",
            "\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8194 - loss: 0.9005 - val_accuracy: 0.8183 - val_loss: 0.8059 - learning_rate: 2.5000e-04\n",
            "Epoch 52/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8237 - loss: 0.8933\n",
            "Epoch 52: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8237 - loss: 0.8933 - val_accuracy: 0.8259 - val_loss: 0.7960 - learning_rate: 1.2500e-04\n",
            "Epoch 53/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8244 - loss: 0.8924\n",
            "Epoch 53: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8244 - loss: 0.8924 - val_accuracy: 0.8211 - val_loss: 0.7934 - learning_rate: 1.2500e-04\n",
            "Epoch 54/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8249 - loss: 0.8886\n",
            "Epoch 54: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8249 - loss: 0.8886 - val_accuracy: 0.8242 - val_loss: 0.7904 - learning_rate: 1.2500e-04\n",
            "Epoch 55/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8253 - loss: 0.8890\n",
            "Epoch 55: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.8253 - loss: 0.8890 - val_accuracy: 0.8236 - val_loss: 0.7933 - learning_rate: 1.2500e-04\n",
            "Epoch 56/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8260 - loss: 0.8871\n",
            "Epoch 56: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8260 - loss: 0.8871 - val_accuracy: 0.8196 - val_loss: 0.7975 - learning_rate: 1.2500e-04\n",
            "Epoch 57/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8252 - loss: 0.8862\n",
            "Epoch 57: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8252 - loss: 0.8862 - val_accuracy: 0.8227 - val_loss: 0.7917 - learning_rate: 1.2500e-04\n",
            "Epoch 58/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8254 - loss: 0.8861\n",
            "Epoch 58: val_accuracy did not improve from 0.82977\n",
            "\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8254 - loss: 0.8861 - val_accuracy: 0.8236 - val_loss: 0.7953 - learning_rate: 1.2500e-04\n",
            "Epoch 59/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8288 - loss: 0.8804\n",
            "Epoch 59: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8287 - loss: 0.8804 - val_accuracy: 0.8213 - val_loss: 0.7969 - learning_rate: 6.2500e-05\n",
            "Epoch 60/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8286 - loss: 0.8794\n",
            "Epoch 60: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8286 - loss: 0.8794 - val_accuracy: 0.8193 - val_loss: 0.8043 - learning_rate: 6.2500e-05\n",
            "Epoch 61/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8284 - loss: 0.8806\n",
            "Epoch 61: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8284 - loss: 0.8806 - val_accuracy: 0.8217 - val_loss: 0.7971 - learning_rate: 6.2500e-05\n",
            "Epoch 62/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8290 - loss: 0.8781\n",
            "Epoch 62: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8290 - loss: 0.8781 - val_accuracy: 0.8202 - val_loss: 0.8004 - learning_rate: 6.2500e-05\n",
            "Epoch 63/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8290 - loss: 0.8794\n",
            "Epoch 63: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.8290 - loss: 0.8794 - val_accuracy: 0.8172 - val_loss: 0.8045 - learning_rate: 6.2500e-05\n",
            "Epoch 64/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8297 - loss: 0.8776\n",
            "Epoch 64: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8297 - loss: 0.8776 - val_accuracy: 0.8171 - val_loss: 0.8013 - learning_rate: 6.2500e-05\n",
            "Epoch 65/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8307 - loss: 0.8771\n",
            "Epoch 65: val_accuracy did not improve from 0.82977\n",
            "\n",
            "Epoch 65: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8307 - loss: 0.8771 - val_accuracy: 0.8258 - val_loss: 0.7925 - learning_rate: 6.2500e-05\n",
            "Epoch 66/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8308 - loss: 0.8754\n",
            "Epoch 66: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8308 - loss: 0.8754 - val_accuracy: 0.8220 - val_loss: 0.7967 - learning_rate: 3.1250e-05\n",
            "Epoch 67/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8316 - loss: 0.8742\n",
            "Epoch 67: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8316 - loss: 0.8742 - val_accuracy: 0.8229 - val_loss: 0.7961 - learning_rate: 3.1250e-05\n",
            "Epoch 68/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8310 - loss: 0.8745\n",
            "Epoch 68: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8310 - loss: 0.8745 - val_accuracy: 0.8241 - val_loss: 0.7947 - learning_rate: 3.1250e-05\n",
            "Epoch 69/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8310 - loss: 0.8739\n",
            "Epoch 69: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8310 - loss: 0.8739 - val_accuracy: 0.8222 - val_loss: 0.7954 - learning_rate: 3.1250e-05\n",
            "Epoch 70/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8316 - loss: 0.8735\n",
            "Epoch 70: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8316 - loss: 0.8735 - val_accuracy: 0.8254 - val_loss: 0.7891 - learning_rate: 3.1250e-05\n",
            "Epoch 71/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8316 - loss: 0.8728\n",
            "Epoch 71: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8316 - loss: 0.8728 - val_accuracy: 0.8252 - val_loss: 0.7926 - learning_rate: 3.1250e-05\n",
            "Epoch 72/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8315 - loss: 0.8727\n",
            "Epoch 72: val_accuracy did not improve from 0.82977\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8315 - loss: 0.8728 - val_accuracy: 0.8243 - val_loss: 0.7936 - learning_rate: 3.1250e-05\n",
            "Epoch 73/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8319 - loss: 0.8724\n",
            "Epoch 73: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8319 - loss: 0.8724 - val_accuracy: 0.8254 - val_loss: 0.7923 - learning_rate: 1.5625e-05\n",
            "Epoch 74/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8326 - loss: 0.8711\n",
            "Epoch 74: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8326 - loss: 0.8711 - val_accuracy: 0.8283 - val_loss: 0.7878 - learning_rate: 1.5625e-05\n",
            "Epoch 75/80\n",
            "\u001b[1m1225/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8322 - loss: 0.8711\n",
            "Epoch 75: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8322 - loss: 0.8711 - val_accuracy: 0.8256 - val_loss: 0.7912 - learning_rate: 1.5625e-05\n",
            "Epoch 76/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8327 - loss: 0.8707\n",
            "Epoch 76: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8327 - loss: 0.8707 - val_accuracy: 0.8250 - val_loss: 0.7920 - learning_rate: 1.5625e-05\n",
            "Epoch 77/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8315 - loss: 0.8704\n",
            "Epoch 77: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8315 - loss: 0.8704 - val_accuracy: 0.8260 - val_loss: 0.7901 - learning_rate: 1.5625e-05\n",
            "Epoch 78/80\n",
            "\u001b[1m1226/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8319 - loss: 0.8712\n",
            "Epoch 78: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8319 - loss: 0.8713 - val_accuracy: 0.8268 - val_loss: 0.7898 - learning_rate: 1.5625e-05\n",
            "Epoch 79/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8319 - loss: 0.8709\n",
            "Epoch 79: val_accuracy did not improve from 0.82977\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8319 - loss: 0.8710 - val_accuracy: 0.8269 - val_loss: 0.7902 - learning_rate: 1.5625e-05\n",
            "Epoch 80/80\n",
            "\u001b[1m1227/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8320 - loss: 0.8708\n",
            "Epoch 80: val_accuracy did not improve from 0.82977\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.8320 - loss: 0.8708 - val_accuracy: 0.8287 - val_loss: 0.7877 - learning_rate: 7.8125e-06\n",
            "\n",
            "âœ… Training Complete.\n",
            "\n",
            "======================================================================\n",
            "ğŸ¯ FINAL EVALUATION (Best Weights)\n",
            "======================================================================\n",
            "ğŸ“¥ Loading best weights from: artifacts/best_semg_net.keras\n",
            "âœ… Best weights loaded successfully.\n",
            "ğŸ† sEMG NET ACCURACY: 0.8136\n",
            "ğŸ† sEMG NET F1 SCORE: 0.8126\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "HIGH-PERFORMANCE PIPELINE: sEMG Net (Deep CNN)\n",
        "for Synapse sEMG Challenge\n",
        "\n",
        "ğŸš€ ARCHITECTURE: sEMG Net\n",
        "1. DEEP CNN: 4-Stage 1D Convolutional Feature Extractor.\n",
        "2. BATCH NORM: Applied after every convolution for stability.\n",
        "3. ROBUSTNESS: High dropout rates to prevent overfitting on noisy EMG.\n",
        "4. TRAINING: Label Smoothing + ReduceLROnPlateau Scheduler.\n",
        "5. STRATEGY: Automatically reloads best weights (Max Val Acc) for testing.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, mixed_precision, callbacks\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from scipy.stats import mode\n",
        "from scipy.signal import butter, filtfilt, iirnotch\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "DATA_DIR = 'data'\n",
        "ARTIFACTS_DIR = 'artifacts'\n",
        "FS = 512\n",
        "EPOCHS = 80\n",
        "BATCH_SIZE = 128\n",
        "RANDOM_SEED = 42\n",
        "VAL_FILE_RATIO = 0.50\n",
        "\n",
        "# Windowing\n",
        "WINDOW_MS = 400\n",
        "STRIDE_MS = 160\n",
        "\n",
        "# Hyperparameters\n",
        "L2_REG = 1e-4\n",
        "DROPOUT_CONV = 0.25\n",
        "DROPOUT_HEAD = 0.5\n",
        "\n",
        "# GPU Setup\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        policy = mixed_precision.Policy('mixed_float16')\n",
        "        mixed_precision.set_global_policy(policy)\n",
        "        print(\"âœ… Mixed Precision (FP16) Enabled\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Set seeds\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# ==================== AUGMENTATION ====================\n",
        "\n",
        "def augment_dataset_advanced(X, y):\n",
        "    print(f\"   âš¡ Augmenting Data (Input: {len(X)} windows)...\")\n",
        "    b, t, c = X.shape\n",
        "\n",
        "    # 1. Channel Masking\n",
        "    X_mask = X.copy()\n",
        "    mask_indices = np.random.choice(b, size=int(b * 0.5), replace=False)\n",
        "    for i in mask_indices:\n",
        "        ch = np.random.randint(0, c)\n",
        "        X_mask[i, :, ch] = 0\n",
        "    X_mask = X_mask + np.random.normal(0, 0.02, size=X_mask.shape)\n",
        "\n",
        "    # 2. MixUp\n",
        "    indices = np.random.permutation(b)\n",
        "    X_shuffled = X[indices]\n",
        "    alpha = 0.2\n",
        "    lam = np.random.beta(alpha, alpha, size=(b, 1, 1))\n",
        "\n",
        "    X_mix = lam * X + (1 - lam) * X_shuffled\n",
        "    y_mix = y.copy()\n",
        "\n",
        "    # Concatenate\n",
        "    X_final = np.concatenate([X, X_mask, X_mix], axis=0)\n",
        "    y_final = np.concatenate([y, y, y_mix], axis=0)\n",
        "\n",
        "    print(f\"   âš¡ Augmentation complete. Size: {len(X_final)} (3x)\")\n",
        "    return X_final, y_final\n",
        "\n",
        "# ==================== PREPROCESSING ====================\n",
        "\n",
        "class SignalPreprocessor:\n",
        "    def __init__(self, fs=1000, bandpass_low=20.0, bandpass_high=450.0, notch_freq=50.0):\n",
        "        self.fs = fs\n",
        "        self.nyq = fs / 2\n",
        "        low = max(0.001, min(bandpass_low / self.nyq, 0.99))\n",
        "        high = max(low + 0.01, min(bandpass_high / self.nyq, 0.999))\n",
        "        self.b_bp, self.a_bp = butter(4, [low, high], btype='band')\n",
        "        self.b_notch, self.a_notch = iirnotch(notch_freq, 30.0, self.fs) if notch_freq > 0 else (None, None)\n",
        "        self.channel_means, self.channel_stds = None, None\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, signals_list):\n",
        "        all_signals = np.concatenate(signals_list, axis=0)\n",
        "        self.channel_means = np.mean(all_signals, axis=0)\n",
        "        self.channel_stds = np.std(all_signals, axis=0) + 1e-8\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, signal):\n",
        "        if len(signal) > 12:\n",
        "            signal = filtfilt(self.b_bp, self.a_bp, signal, axis=0)\n",
        "            if self.b_notch is not None:\n",
        "                signal = filtfilt(self.b_notch, self.a_notch, signal, axis=0)\n",
        "        if self.fitted:\n",
        "            return (signal - self.channel_means) / self.channel_stds\n",
        "        return (signal - np.mean(signal, axis=0)) / (np.std(signal, axis=0) + 1e-8)\n",
        "\n",
        "    def segment(self, signal, window_ms=200, stride_ms=100):\n",
        "        win_sz = int(window_ms * self.fs / 1000)\n",
        "        step = int(stride_ms * self.fs / 1000)\n",
        "        n = len(signal)\n",
        "        if n < win_sz: return None\n",
        "        n_win = (n - win_sz) // step + 1\n",
        "        idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
        "        return signal[idx]\n",
        "\n",
        "# ==================== sEMG NET ARCHITECTURE ====================\n",
        "\n",
        "def conv_block(x, filters, kernel_size, pool=True):\n",
        "    \"\"\" Standard Conv Block: Conv -> BN -> ReLU -> Dropout -> Pool \"\"\"\n",
        "    x = layers.Conv1D(filters, kernel_size, padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dropout(DROPOUT_CONV)(x)\n",
        "    if pool:\n",
        "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    return x\n",
        "\n",
        "def make_semg_net(input_shape, n_classes):\n",
        "    \"\"\"\n",
        "    sEMG Net: A classic Deep CNN for EMG Time-Series.\n",
        "    Focuses on extracting hierarchical temporal features.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # 1. Noise Injection for robustness\n",
        "    x = layers.GaussianNoise(0.05)(inputs)\n",
        "\n",
        "    # 2. Block 1: Capture Raw Temporal Features (Large Kernel)\n",
        "    x = conv_block(x, filters=64, kernel_size=9, pool=False)\n",
        "\n",
        "    # 3. Block 2: Feature Extraction\n",
        "    x = conv_block(x, filters=128, kernel_size=5, pool=True)\n",
        "\n",
        "    # 4. Block 3: Deep Features\n",
        "    x = conv_block(x, filters=256, kernel_size=3, pool=True)\n",
        "\n",
        "    # 5. Block 4: Abstract Features\n",
        "    x = conv_block(x, filters=512, kernel_size=3, pool=True)\n",
        "\n",
        "    # 6. Global Context\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # 7. Classification Head\n",
        "    x = layers.Dense(512, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
        "    x = layers.Dropout(DROPOUT_HEAD)(x)\n",
        "\n",
        "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs, name='sEMG_Net')\n",
        "\n",
        "# ==================== UTILS ====================\n",
        "\n",
        "def get_session_files(data_dir, sessions):\n",
        "    files = []\n",
        "    for session in sessions:\n",
        "        pattern = f'{data_dir}/**/{session}/**/*.csv'\n",
        "        files.extend(sorted(glob.glob(pattern, recursive=True)))\n",
        "    return files\n",
        "\n",
        "def split_files_by_ratio(files, val_ratio, seed=RANDOM_SEED):\n",
        "    gesture_files = {}\n",
        "    for f in files:\n",
        "        match = re.search(r'gesture(\\d+)', f)\n",
        "        if match:\n",
        "            g = int(match.group(1))\n",
        "            gesture_files.setdefault(g, []).append(f)\n",
        "    train, val = [], []\n",
        "    rng = random.Random(seed)\n",
        "    for g, gfiles in gesture_files.items():\n",
        "        rng.shuffle(gfiles)\n",
        "        n_val = max(1, int(len(gfiles) * val_ratio))\n",
        "        val.extend(gfiles[:n_val])\n",
        "        train.extend(gfiles[n_val:])\n",
        "    return train, val\n",
        "\n",
        "def load_files_data(file_list):\n",
        "    data_list, labels_list = [], []\n",
        "    for f in file_list:\n",
        "        try:\n",
        "            lbl = int(re.search(r'gesture(\\d+)', f).group(1))\n",
        "            d = pd.read_csv(f).values\n",
        "            if d.shape[1] >= 8:\n",
        "                data_list.append(d)\n",
        "                labels_list.append(np.full(len(d), lbl))\n",
        "        except: pass\n",
        "    return data_list, labels_list\n",
        "\n",
        "def window_data(data_list, labels_list, prep, window_ms, stride_ms):\n",
        "    X_wins, y_wins = [], []\n",
        "    win_sz = int(window_ms * FS / 1000)\n",
        "    step = int(stride_ms * FS / 1000)\n",
        "    for d, l in zip(data_list, labels_list):\n",
        "        d_filt = prep.transform(d)\n",
        "        w = prep.segment(d_filt, window_ms, stride_ms)\n",
        "        if w is not None:\n",
        "            X_wins.append(w)\n",
        "            n_win = (len(d) - win_sz) // step + 1\n",
        "            idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
        "            w_modes = mode(l[idx], axis=1, keepdims=True)[0].flatten()\n",
        "            y_wins.append(w_modes)\n",
        "    if not X_wins: return None, None\n",
        "    return np.concatenate(X_wins), np.concatenate(y_wins)\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "\n",
        "def main():\n",
        "    os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "    print(\"=\"*70 + \"\\nğŸš€ TRAIN: sEMG NET (Deep CNN)\\n\" + \"=\"*70)\n",
        "\n",
        "    # 1. Load Data\n",
        "    existing_csvs = glob.glob(f'{DATA_DIR}/**/*.csv', recursive=True)\n",
        "    if not existing_csvs:\n",
        "        import gdown, zipfile\n",
        "        gdown.download('https://drive.google.com/uc?id=16iNEwhThf2LcX7rOOVM03MTZiwq7G51x', 'dataset.zip', quiet=False)\n",
        "        with zipfile.ZipFile('dataset.zip', 'r') as z: z.extractall(DATA_DIR)\n",
        "        os.remove('dataset.zip')\n",
        "\n",
        "    train_files = get_session_files(DATA_DIR, ['Session1', 'Session2'])\n",
        "    session3_files = get_session_files(DATA_DIR, ['Session3'])\n",
        "    val_files, test_files = split_files_by_ratio(session3_files, VAL_FILE_RATIO)\n",
        "\n",
        "    print(f\"\\n[Data Loaded] Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}\")\n",
        "\n",
        "    # 2. Raw Data\n",
        "    train_data, train_labels = load_files_data(train_files)\n",
        "    val_data, val_labels = load_files_data(val_files)\n",
        "    test_data, test_labels = load_files_data(test_files)\n",
        "\n",
        "    # 3. Preprocess\n",
        "    prep = SignalPreprocessor(fs=FS)\n",
        "    prep.fit(train_data)\n",
        "\n",
        "    print(\"\\n[Windowing] Processing...\")\n",
        "    X_train, y_train_raw = window_data(train_data, train_labels, prep, WINDOW_MS, STRIDE_MS)\n",
        "    X_val, y_val_raw = window_data(val_data, val_labels, prep, WINDOW_MS, STRIDE_MS)\n",
        "    X_test, y_test_raw = window_data(test_data, test_labels, prep, WINDOW_MS, STRIDE_MS)\n",
        "\n",
        "    # 4. Augmentation\n",
        "    X_train, y_train_raw = augment_dataset_advanced(X_train, y_train_raw)\n",
        "\n",
        "    # 5. Encoding\n",
        "    le = LabelEncoder()\n",
        "    le.fit(y_train_raw)\n",
        "    y_train = le.transform(y_train_raw)\n",
        "\n",
        "    def safe_transform(enc, labels):\n",
        "        return np.array([enc.transform([l])[0] if l in enc.classes_ else -1 for l in labels])\n",
        "\n",
        "    y_val = safe_transform(le, y_val_raw)\n",
        "    y_test = safe_transform(le, y_test_raw)\n",
        "    n_classes = len(le.classes_)\n",
        "    input_shape = X_train.shape[1:]\n",
        "\n",
        "    # ==================== TRAINING PHASE ====================\n",
        "    print(\"\\n\" + \"-\"*40 + \"\\nğŸ‹ï¸ TRAIN MODEL: sEMG NET\\n\" + \"-\"*40)\n",
        "\n",
        "    # One-Hot Encoding for Label Smoothing\n",
        "    y_train_hot = tf.keras.utils.to_categorical(y_train, n_classes)\n",
        "    y_val_hot = tf.keras.utils.to_categorical(y_val, n_classes)\n",
        "\n",
        "    # Label Smoothing\n",
        "    loss_fn = keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0.1)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # âœ… OPTIMIZER & LEARNING RATE SCHEDULER (UPDATED)\n",
        "    # ------------------------------------------------------------------\n",
        "    # Start with a fixed learning rate\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    # Scheduler: Reduce LR if validation accuracy plateaus\n",
        "    lr_scheduler = callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',  # Watch validation accuracy\n",
        "        mode='max',              # We want it to be maximized\n",
        "        factor=0.5,              # Reduce LR by 50%\n",
        "        patience=7,              # Wait 7 epochs of no improvement\n",
        "        min_lr=1e-6,             # Lower bound\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Checkpoint Strategy: Save only the best weights\n",
        "    best_model_path = f'{ARTIFACTS_DIR}/best_semg_net.keras'\n",
        "    checkpoint_cb = callbacks.ModelCheckpoint(\n",
        "        filepath=best_model_path,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Initialize sEMG Net\n",
        "    model = make_semg_net(input_shape, n_classes)\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Train (Added lr_scheduler to callbacks)\n",
        "    model.fit(X_train, y_train_hot,\n",
        "              validation_data=(X_val, y_val_hot),\n",
        "              epochs=EPOCHS,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              callbacks=[checkpoint_cb, lr_scheduler], # <--- Scheduler added here\n",
        "              verbose=1)\n",
        "\n",
        "    print(\"\\nâœ… Training Complete.\")\n",
        "\n",
        "    # ==================== EVALUATION ====================\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\nğŸ¯ FINAL EVALUATION (Best Weights)\\n\" + \"=\"*70)\n",
        "\n",
        "    # Load Best Weights\n",
        "    print(f\"ğŸ“¥ Loading best weights from: {best_model_path}\")\n",
        "    try:\n",
        "        model.load_weights(best_model_path)\n",
        "        print(\"âœ… Best weights loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Warning: Could not load best weights ({e}). Using last epoch weights.\")\n",
        "\n",
        "    test_probs = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "    test_preds = test_probs.argmax(axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_test, test_preds)\n",
        "    f1 = f1_score(y_test, test_preds, average='macro')\n",
        "\n",
        "    print(f\"ğŸ† sEMG NET ACCURACY: {acc:.4f}\")\n",
        "    print(f\"ğŸ† sEMG NET F1 SCORE: {f1:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ğŸ† ENSEMBLE PIPELINE: Inception-SE + sEMG-Net\n",
        "for Synapse sEMG Challenge\n",
        "\n",
        "INTEGRATED ARCHITECTURES:\n",
        "1. Model A: Inception-SE-TCN (Multi-scale + Channel Attention + Transformers)\n",
        "2. Model B: sEMG Net (Deep CNN Feature Extractor)\n",
        "\n",
        "STRATEGY:\n",
        "- Shared Data Pipeline (Filtering, Windowing, MixUp/Masking Augmentation).\n",
        "- Sequential Training: Trains models A and B and saves the BEST weights for each.\n",
        "- Ensemble Inference: Soft Voting (Average Probability) of the top 2 models.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, mixed_precision, callbacks\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from scipy.stats import mode\n",
        "from scipy.signal import butter, filtfilt, iirnotch\n",
        "import warnings\n",
        "import gc\n",
        "\n",
        "# Standardize Environment\n",
        "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "DATA_DIR = 'data'\n",
        "ARTIFACTS_DIR = 'artifacts'\n",
        "FS = 512\n",
        "EPOCHS = 80\n",
        "BATCH_SIZE = 128\n",
        "RANDOM_SEED = 42\n",
        "VAL_FILE_RATIO = 0.50\n",
        "\n",
        "# Windowing\n",
        "WINDOW_MS = 400\n",
        "STRIDE_MS = 160\n",
        "\n",
        "# Hyperparameters\n",
        "L2_REG = 1e-4\n",
        "\n",
        "# GPU Setup\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        policy = mixed_precision.Policy('mixed_float16')\n",
        "        mixed_precision.set_global_policy(policy)\n",
        "        print(\"âœ… Mixed Precision (FP16) Enabled\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Set seeds\n",
        "def set_global_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_global_seeds(RANDOM_SEED)\n",
        "\n",
        "# ==================== 1. PREPROCESSING & AUGMENTATION ====================\n",
        "\n",
        "class SignalPreprocessor:\n",
        "    def __init__(self, fs=1000, bandpass_low=20.0, bandpass_high=450.0, notch_freq=50.0):\n",
        "        self.fs = fs\n",
        "        nyq = fs / 2\n",
        "        low = max(0.001, min(bandpass_low / nyq, 0.99))\n",
        "        high = max(low + 0.01, min(bandpass_high / nyq, 0.999))\n",
        "        self.b_bp, self.a_bp = butter(4, [low, high], btype='band')\n",
        "        self.b_notch, self.a_notch = iirnotch(notch_freq, 30.0, self.fs) if notch_freq > 0 else (None, None)\n",
        "        self.channel_means, self.channel_stds = None, None\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, signals_list):\n",
        "        all_signals = np.concatenate(signals_list, axis=0)\n",
        "        self.channel_means = np.mean(all_signals, axis=0)\n",
        "        self.channel_stds = np.std(all_signals, axis=0) + 1e-8\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, signal):\n",
        "        if len(signal) > 12:\n",
        "            signal = filtfilt(self.b_bp, self.a_bp, signal, axis=0)\n",
        "            if self.b_notch is not None:\n",
        "                signal = filtfilt(self.b_notch, self.a_notch, signal, axis=0)\n",
        "        if self.fitted:\n",
        "            return (signal - self.channel_means) / self.channel_stds\n",
        "        return (signal - np.mean(signal, axis=0)) / (np.std(signal, axis=0) + 1e-8)\n",
        "\n",
        "    def segment(self, signal, window_ms=200, stride_ms=100):\n",
        "        win_sz = int(window_ms * self.fs / 1000)\n",
        "        step = int(stride_ms * self.fs / 1000)\n",
        "        n = len(signal)\n",
        "        if n < win_sz: return None\n",
        "        n_win = (n - win_sz) // step + 1\n",
        "        idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
        "        return signal[idx]\n",
        "\n",
        "def augment_dataset_advanced(X, y):\n",
        "    print(f\"    âš¡ Augmenting Data (Input: {len(X)} windows)...\")\n",
        "    b, t, c = X.shape\n",
        "\n",
        "    # 1. Channel Masking\n",
        "    X_mask = X.copy()\n",
        "    mask_indices = np.random.choice(b, size=int(b * 0.5), replace=False)\n",
        "    for i in mask_indices:\n",
        "        ch = np.random.randint(0, c)\n",
        "        X_mask[i, :, ch] = 0\n",
        "    X_mask = X_mask + np.random.normal(0, 0.02, size=X_mask.shape)\n",
        "\n",
        "    # 2. MixUp\n",
        "    indices = np.random.permutation(b)\n",
        "    X_shuffled = X[indices]\n",
        "    alpha = 0.2\n",
        "    lam = np.random.beta(alpha, alpha, size=(b, 1, 1))\n",
        "    X_mix = lam * X + (1 - lam) * X_shuffled\n",
        "    y_mix = y.copy()\n",
        "\n",
        "    # Concatenate\n",
        "    X_final = np.concatenate([X, X_mask, X_mix], axis=0)\n",
        "    y_final = np.concatenate([y, y, y_mix], axis=0)\n",
        "    print(f\"    âš¡ Augmentation complete. Size: {len(X_final)} (3x)\")\n",
        "    return X_final, y_final\n",
        "\n",
        "def get_session_files(data_dir, sessions):\n",
        "    files = []\n",
        "    for session in sessions:\n",
        "        pattern = f'{data_dir}/**/{session}/**/*.csv'\n",
        "        files.extend(sorted(glob.glob(pattern, recursive=True)))\n",
        "    return files\n",
        "\n",
        "def split_files_by_ratio(files, val_ratio, seed=RANDOM_SEED):\n",
        "    gesture_files = {}\n",
        "    for f in files:\n",
        "        match = re.search(r'gesture(\\d+)', f)\n",
        "        if match:\n",
        "            g = int(match.group(1))\n",
        "            gesture_files.setdefault(g, []).append(f)\n",
        "    train, val = [], []\n",
        "    rng = random.Random(seed)\n",
        "    for g, gfiles in gesture_files.items():\n",
        "        rng.shuffle(gfiles)\n",
        "        n_val = max(1, int(len(gfiles) * val_ratio))\n",
        "        val.extend(gfiles[:n_val])\n",
        "        train.extend(gfiles[n_val:])\n",
        "    return train, val\n",
        "\n",
        "def load_files_data(file_list):\n",
        "    data_list, labels_list = [], []\n",
        "    for f in file_list:\n",
        "        try:\n",
        "            lbl = int(re.search(r'gesture(\\d+)', f).group(1))\n",
        "            d = pd.read_csv(f).values\n",
        "            if d.shape[1] >= 8:\n",
        "                data_list.append(d)\n",
        "                labels_list.append(np.full(len(d), lbl))\n",
        "        except: pass\n",
        "    return data_list, labels_list\n",
        "\n",
        "def window_data(data_list, labels_list, prep, window_ms, stride_ms):\n",
        "    X_wins, y_wins = [], []\n",
        "    win_sz = int(window_ms * FS / 1000)\n",
        "    step = int(stride_ms * FS / 1000)\n",
        "    for d, l in zip(data_list, labels_list):\n",
        "        d_filt = prep.transform(d)\n",
        "        w = prep.segment(d_filt, window_ms, stride_ms)\n",
        "        if w is not None:\n",
        "            X_wins.append(w)\n",
        "            n_win = (len(d) - win_sz) // step + 1\n",
        "            idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
        "            w_modes = mode(l[idx], axis=1, keepdims=True)[0].flatten()\n",
        "            y_wins.append(w_modes)\n",
        "    if not X_wins: return None, None\n",
        "    return np.concatenate(X_wins), np.concatenate(y_wins)\n",
        "\n",
        "# ==================== 2. MODEL DEFINITIONS ====================\n",
        "\n",
        "# --- MODEL 1: INCEPTION-SE-TCN ---\n",
        "def squeeze_excite_block(input_tensor, ratio=8):\n",
        "    filters = input_tensor.shape[-1]\n",
        "    se = layers.GlobalAveragePooling1D()(input_tensor)\n",
        "    se = layers.Dense(filters // ratio, activation='relu', kernel_regularizer=l2(L2_REG))(se)\n",
        "    se = layers.Dense(filters, activation='sigmoid', kernel_regularizer=l2(L2_REG))(se)\n",
        "    se = layers.Reshape((1, filters))(se)\n",
        "    return layers.Multiply()([input_tensor, se])\n",
        "\n",
        "def inception_block(x, filters, dilation_rate):\n",
        "    b1 = layers.Conv1D(filters=filters//2, kernel_size=3, dilation_rate=dilation_rate,\n",
        "                       padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
        "    b1 = layers.BatchNormalization()(b1)\n",
        "    b1 = layers.Activation('relu')(b1)\n",
        "    b2 = layers.Conv1D(filters=filters//2, kernel_size=7, dilation_rate=dilation_rate,\n",
        "                       padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
        "    b2 = layers.BatchNormalization()(b2)\n",
        "    b2 = layers.Activation('relu')(b2)\n",
        "    return layers.Concatenate()([b1, b2])\n",
        "\n",
        "def make_inception_se_tcn(input_shape, n_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.GaussianNoise(0.05)(inputs)\n",
        "    filters = 64\n",
        "    for dilation_rate in [1, 2, 4, 8]:\n",
        "        prev_x = x\n",
        "        x = inception_block(x, filters, dilation_rate)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        x = squeeze_excite_block(x, ratio=8)\n",
        "        if prev_x.shape[-1] != filters:\n",
        "            prev_x = layers.Conv1D(filters=filters, kernel_size=1, padding='same')(prev_x)\n",
        "        x = layers.Add()([x, prev_x])\n",
        "\n",
        "    # Self-Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = layers.MultiHeadAttention(key_dim=64, num_heads=4, dropout=0.3)(x, x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
        "    return keras.Model(inputs, outputs, name='Inception_SE_Attn')\n",
        "\n",
        "# --- MODEL 2: sEMG NET (Deep CNN) ---\n",
        "def conv_block(x, filters, kernel_size, pool=True):\n",
        "    x = layers.Conv1D(filters, kernel_size, padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    if pool: x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    return x\n",
        "\n",
        "def make_semg_net(input_shape, n_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.GaussianNoise(0.05)(inputs)\n",
        "    x = conv_block(x, 64, 9, pool=False)\n",
        "    x = conv_block(x, 128, 5, pool=True)\n",
        "    x = conv_block(x, 256, 3, pool=True)\n",
        "    x = conv_block(x, 512, 3, pool=True)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(512, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
        "    return keras.Model(inputs, outputs, name='sEMG_Net')\n",
        "\n",
        "# ==================== 3. TRAINING & ENSEMBLE LOGIC ====================\n",
        "\n",
        "def train_and_save(model_builder, model_name, X_train, y_train_hot, X_val, y_val_hot, input_shape, n_classes):\n",
        "    \"\"\" Generic training function for all models \"\"\"\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(f\"ğŸ‹ï¸ TRAIN: {model_name.upper()}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    # Cosine Decay\n",
        "    total_steps = len(X_train) // BATCH_SIZE * EPOCHS\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
        "        initial_learning_rate=0.001, first_decay_steps=int(total_steps * 0.3),\n",
        "        t_mul=2.0, m_mul=0.9, alpha=1e-5\n",
        "    )\n",
        "\n",
        "    # Save Best Model (Higher val_accuracy)\n",
        "    save_path = f'{ARTIFACTS_DIR}/best_{model_name}.keras'\n",
        "    checkpoint = callbacks.ModelCheckpoint(\n",
        "        save_path, monitor='val_accuracy', mode='max',\n",
        "        save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = model_builder(input_shape, n_classes)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "                  loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train\n",
        "    model.fit(X_train, y_train_hot,\n",
        "              validation_data=(X_val, y_val_hot),\n",
        "              epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "              callbacks=[checkpoint], verbose=1)\n",
        "\n",
        "    # Clear Memory\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    print(f\"âœ… {model_name} training complete. Best model saved.\")\n",
        "    return save_path\n",
        "\n",
        "def main():\n",
        "    os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "    print(\"=\"*70 + \"\\nğŸš€ ENSEMBLE PIPELINE START (TCN + sEMG-Net)\\n\" + \"=\"*70)\n",
        "\n",
        "    # 1. DATA LOADING\n",
        "    existing_csvs = glob.glob(f'{DATA_DIR}/**/*.csv', recursive=True)\n",
        "    if not existing_csvs:\n",
        "        print(\"ğŸ“¥ Downloading Dataset...\")\n",
        "        import gdown, zipfile\n",
        "        gdown.download('https://drive.google.com/uc?id=16iNEwhThf2LcX7rOOVM03MTZiwq7G51x', 'dataset.zip', quiet=False)\n",
        "        with zipfile.ZipFile('dataset.zip', 'r') as z: z.extractall(DATA_DIR)\n",
        "        os.remove('dataset.zip')\n",
        "\n",
        "    train_files = get_session_files(DATA_DIR, ['Session1', 'Session2'])\n",
        "    session3_files = get_session_files(DATA_DIR, ['Session3'])\n",
        "    val_files, test_files = split_files_by_ratio(session3_files, VAL_FILE_RATIO)\n",
        "\n",
        "    print(f\"[Data] Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}\")\n",
        "\n",
        "    train_data, train_labels = load_files_data(train_files)\n",
        "    val_data, val_labels = load_files_data(val_files)\n",
        "    test_data, test_labels = load_files_data(test_files)\n",
        "\n",
        "    # 2. PREPROCESSING\n",
        "    prep = SignalPreprocessor(fs=FS).fit(train_data)\n",
        "    X_train, y_train_raw = window_data(train_data, train_labels, prep, WINDOW_MS, STRIDE_MS)\n",
        "    X_val, y_val_raw = window_data(val_data, val_labels, prep, WINDOW_MS, STRIDE_MS)\n",
        "    X_test, y_test_raw = window_data(test_data, test_labels, prep, WINDOW_MS, STRIDE_MS)\n",
        "\n",
        "    # 3. AUGMENTATION (Train Only)\n",
        "    X_train, y_train_raw = augment_dataset_advanced(X_train, y_train_raw)\n",
        "\n",
        "    # 4. ENCODING\n",
        "    le = LabelEncoder().fit(y_train_raw)\n",
        "    y_train = le.transform(y_train_raw)\n",
        "    y_val = np.array([le.transform([l])[0] if l in le.classes_ else -1 for l in y_val_raw])\n",
        "    y_test = np.array([le.transform([l])[0] if l in le.classes_ else -1 for l in y_test_raw])\n",
        "\n",
        "    n_classes = len(le.classes_)\n",
        "    input_shape = X_train.shape[1:]\n",
        "\n",
        "    y_train_hot = tf.keras.utils.to_categorical(y_train, n_classes)\n",
        "    y_val_hot = tf.keras.utils.to_categorical(y_val, n_classes)\n",
        "\n",
        "    # ==================== 5. SEQUENTIAL TRAINING ====================\n",
        "\n",
        "    # Train Model 1: Inception-SE-TCN\n",
        "    path_inception = train_and_save(make_inception_se_tcn, \"inception_se\",\n",
        "                                    X_train, y_train_hot, X_val, y_val_hot, input_shape, n_classes)\n",
        "\n",
        "    # Train Model 2: sEMG Net\n",
        "    path_semgnet = train_and_save(make_semg_net, \"semg_net\",\n",
        "                                  X_train, y_train_hot, X_val, y_val_hot, input_shape, n_classes)\n",
        "\n",
        "    # ==================== 6. ENSEMBLE EVALUATION ====================\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\nğŸ¯ ENSEMBLE EVALUATION (2-Model Soft Voting)\\n\" + \"=\"*70)\n",
        "\n",
        "    # Load Models (Best Weights)\n",
        "    m1 = make_inception_se_tcn(input_shape, n_classes)\n",
        "    m1.load_weights(path_inception)\n",
        "\n",
        "    m2 = make_semg_net(input_shape, n_classes)\n",
        "    m2.load_weights(path_semgnet)\n",
        "\n",
        "    print(\"âš¡ Predicting with Model 1 (Inception-SE-TCN)...\")\n",
        "    p1 = m1.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "    print(\"âš¡ Predicting with Model 2 (sEMG Net)...\")\n",
        "    p2 = m2.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "    # --- ENSEMBLE AGGREGATION (Average Probabilities) ---\n",
        "    ensemble_probs = (p1 + p2) / 2.0\n",
        "    ensemble_preds = ensemble_probs.argmax(axis=1)\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_test, ensemble_preds)\n",
        "    f1 = f1_score(y_test, ensemble_preds, average='macro')\n",
        "\n",
        "    print(\"\\n\" + \"*\"*40)\n",
        "    print(f\"ğŸ† FINAL ENSEMBLE ACCURACY: {acc:.4f}\")\n",
        "    print(f\"ğŸ† FINAL ENSEMBLE F1 SCORE: {f1:.4f}\")\n",
        "    print(\"*\"*40)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdK7pmXmYHC9",
        "outputId": "4a37138c-afba-448b-e631-77e4e7d8a410"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Mixed Precision (FP16) Enabled\n",
            "======================================================================\n",
            "ğŸš€ ENSEMBLE PIPELINE START (TCN + sEMG-Net)\n",
            "======================================================================\n",
            "ğŸ“¥ Downloading Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16iNEwhThf2LcX7rOOVM03MTZiwq7G51x\n",
            "From (redirected): https://drive.google.com/uc?id=16iNEwhThf2LcX7rOOVM03MTZiwq7G51x&confirm=t&uuid=e6f26970-0f35-45ec-ac9e-4705f32c613a\n",
            "To: /content/dataset.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 489M/489M [00:03<00:00, 150MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Data] Train: 1750 | Val: 440 | Test: 435\n",
            "    âš¡ Augmenting Data (Input: 52500 windows)...\n",
            "    âš¡ Augmentation complete. Size: 157500 (3x)\n",
            "\n",
            "--------------------------------------------------\n",
            "ğŸ‹ï¸ TRAIN: INCEPTION_SE\n",
            "--------------------------------------------------\n",
            "Epoch 1/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5676 - loss: 1.2665\n",
            "Epoch 1: val_accuracy improved from -inf to 0.75386, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 41ms/step - accuracy: 0.5677 - loss: 1.2665 - val_accuracy: 0.7539 - val_loss: 0.8877\n",
            "Epoch 2/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7041 - loss: 1.0894\n",
            "Epoch 2: val_accuracy improved from 0.75386 to 0.76583, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7041 - loss: 1.0893 - val_accuracy: 0.7658 - val_loss: 0.8851\n",
            "Epoch 3/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7322 - loss: 1.0470\n",
            "Epoch 3: val_accuracy improved from 0.76583 to 0.76856, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7322 - loss: 1.0470 - val_accuracy: 0.7686 - val_loss: 0.8623\n",
            "Epoch 4/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7482 - loss: 1.0232\n",
            "Epoch 4: val_accuracy improved from 0.76856 to 0.78970, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7482 - loss: 1.0232 - val_accuracy: 0.7897 - val_loss: 0.8488\n",
            "Epoch 5/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7586 - loss: 1.0090\n",
            "Epoch 5: val_accuracy did not improve from 0.78970\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7586 - loss: 1.0090 - val_accuracy: 0.7674 - val_loss: 0.8847\n",
            "Epoch 6/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7668 - loss: 0.9979\n",
            "Epoch 6: val_accuracy improved from 0.78970 to 0.79447, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7668 - loss: 0.9979 - val_accuracy: 0.7945 - val_loss: 0.8322\n",
            "Epoch 7/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7745 - loss: 0.9855\n",
            "Epoch 7: val_accuracy improved from 0.79447 to 0.80318, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7745 - loss: 0.9855 - val_accuracy: 0.8032 - val_loss: 0.8134\n",
            "Epoch 8/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7804 - loss: 0.9774\n",
            "Epoch 8: val_accuracy did not improve from 0.80318\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7804 - loss: 0.9774 - val_accuracy: 0.7698 - val_loss: 0.8759\n",
            "Epoch 9/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7869 - loss: 0.9661\n",
            "Epoch 9: val_accuracy did not improve from 0.80318\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7869 - loss: 0.9661 - val_accuracy: 0.7984 - val_loss: 0.8255\n",
            "Epoch 10/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7911 - loss: 0.9615\n",
            "Epoch 10: val_accuracy did not improve from 0.80318\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7911 - loss: 0.9615 - val_accuracy: 0.7998 - val_loss: 0.8257\n",
            "Epoch 11/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7951 - loss: 0.9539\n",
            "Epoch 11: val_accuracy improved from 0.80318 to 0.80371, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7951 - loss: 0.9539 - val_accuracy: 0.8037 - val_loss: 0.8182\n",
            "Epoch 12/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7986 - loss: 0.9482\n",
            "Epoch 12: val_accuracy improved from 0.80371 to 0.81273, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7986 - loss: 0.9482 - val_accuracy: 0.8127 - val_loss: 0.8121\n",
            "Epoch 13/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8041 - loss: 0.9383\n",
            "Epoch 13: val_accuracy improved from 0.81273 to 0.81902, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8041 - loss: 0.9383 - val_accuracy: 0.8190 - val_loss: 0.7964\n",
            "Epoch 14/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8067 - loss: 0.9334\n",
            "Epoch 14: val_accuracy did not improve from 0.81902\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8067 - loss: 0.9334 - val_accuracy: 0.8074 - val_loss: 0.8181\n",
            "Epoch 15/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8105 - loss: 0.9267\n",
            "Epoch 15: val_accuracy did not improve from 0.81902\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8105 - loss: 0.9267 - val_accuracy: 0.8160 - val_loss: 0.8069\n",
            "Epoch 16/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8129 - loss: 0.9218\n",
            "Epoch 16: val_accuracy improved from 0.81902 to 0.82295, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8129 - loss: 0.9218 - val_accuracy: 0.8230 - val_loss: 0.7879\n",
            "Epoch 17/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8157 - loss: 0.9163\n",
            "Epoch 17: val_accuracy improved from 0.82295 to 0.82333, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8157 - loss: 0.9163 - val_accuracy: 0.8233 - val_loss: 0.7983\n",
            "Epoch 18/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8189 - loss: 0.9128\n",
            "Epoch 18: val_accuracy improved from 0.82333 to 0.82371, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8189 - loss: 0.9128 - val_accuracy: 0.8237 - val_loss: 0.7846\n",
            "Epoch 19/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8205 - loss: 0.9075\n",
            "Epoch 19: val_accuracy improved from 0.82371 to 0.82750, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8205 - loss: 0.9075 - val_accuracy: 0.8275 - val_loss: 0.7871\n",
            "Epoch 20/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8218 - loss: 0.9032\n",
            "Epoch 20: val_accuracy improved from 0.82750 to 0.83000, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8218 - loss: 0.9032 - val_accuracy: 0.8300 - val_loss: 0.7805\n",
            "Epoch 21/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8237 - loss: 0.8995\n",
            "Epoch 21: val_accuracy improved from 0.83000 to 0.83030, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8237 - loss: 0.8996 - val_accuracy: 0.8303 - val_loss: 0.7823\n",
            "Epoch 22/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8255 - loss: 0.8982\n",
            "Epoch 22: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8255 - loss: 0.8982 - val_accuracy: 0.8288 - val_loss: 0.7848\n",
            "Epoch 23/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8266 - loss: 0.8960\n",
            "Epoch 23: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8266 - loss: 0.8960 - val_accuracy: 0.8300 - val_loss: 0.7866\n",
            "Epoch 24/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8264 - loss: 0.8955\n",
            "Epoch 24: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8264 - loss: 0.8955 - val_accuracy: 0.8095 - val_loss: 0.8131\n",
            "Epoch 25/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8002 - loss: 0.9425\n",
            "Epoch 25: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8002 - loss: 0.9425 - val_accuracy: 0.8080 - val_loss: 0.8295\n",
            "Epoch 26/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8016 - loss: 0.9451\n",
            "Epoch 26: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8016 - loss: 0.9451 - val_accuracy: 0.8028 - val_loss: 0.8315\n",
            "Epoch 27/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8018 - loss: 0.9428\n",
            "Epoch 27: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8017 - loss: 0.9428 - val_accuracy: 0.8237 - val_loss: 0.7809\n",
            "Epoch 28/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8020 - loss: 0.9438\n",
            "Epoch 28: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8020 - loss: 0.9438 - val_accuracy: 0.8108 - val_loss: 0.8166\n",
            "Epoch 29/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8040 - loss: 0.9420\n",
            "Epoch 29: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8040 - loss: 0.9420 - val_accuracy: 0.8115 - val_loss: 0.8185\n",
            "Epoch 30/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8047 - loss: 0.9387\n",
            "Epoch 30: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8047 - loss: 0.9388 - val_accuracy: 0.8102 - val_loss: 0.8240\n",
            "Epoch 31/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8065 - loss: 0.9383\n",
            "Epoch 31: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8065 - loss: 0.9383 - val_accuracy: 0.8059 - val_loss: 0.8274\n",
            "Epoch 32/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8082 - loss: 0.9353\n",
            "Epoch 32: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8082 - loss: 0.9354 - val_accuracy: 0.8080 - val_loss: 0.8225\n",
            "Epoch 33/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8085 - loss: 0.9369\n",
            "Epoch 33: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8085 - loss: 0.9369 - val_accuracy: 0.8214 - val_loss: 0.8199\n",
            "Epoch 34/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8100 - loss: 0.9320\n",
            "Epoch 34: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8099 - loss: 0.9321 - val_accuracy: 0.8070 - val_loss: 0.8216\n",
            "Epoch 35/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8123 - loss: 0.9324\n",
            "Epoch 35: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8123 - loss: 0.9324 - val_accuracy: 0.8077 - val_loss: 0.8262\n",
            "Epoch 36/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8134 - loss: 0.9286\n",
            "Epoch 36: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8134 - loss: 0.9286 - val_accuracy: 0.8062 - val_loss: 0.8285\n",
            "Epoch 37/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8137 - loss: 0.9278\n",
            "Epoch 37: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8137 - loss: 0.9278 - val_accuracy: 0.8120 - val_loss: 0.8370\n",
            "Epoch 38/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8141 - loss: 0.9258\n",
            "Epoch 38: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8141 - loss: 0.9258 - val_accuracy: 0.8199 - val_loss: 0.8173\n",
            "Epoch 39/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8159 - loss: 0.9244\n",
            "Epoch 39: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8159 - loss: 0.9244 - val_accuracy: 0.8094 - val_loss: 0.8315\n",
            "Epoch 40/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8180 - loss: 0.9193\n",
            "Epoch 40: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8180 - loss: 0.9193 - val_accuracy: 0.8161 - val_loss: 0.8183\n",
            "Epoch 41/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8184 - loss: 0.9170\n",
            "Epoch 41: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8184 - loss: 0.9170 - val_accuracy: 0.8235 - val_loss: 0.7951\n",
            "Epoch 42/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8203 - loss: 0.9175\n",
            "Epoch 42: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8203 - loss: 0.9175 - val_accuracy: 0.8127 - val_loss: 0.8319\n",
            "Epoch 43/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8218 - loss: 0.9145\n",
            "Epoch 43: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8218 - loss: 0.9145 - val_accuracy: 0.8137 - val_loss: 0.8212\n",
            "Epoch 44/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8216 - loss: 0.9117\n",
            "Epoch 44: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8216 - loss: 0.9117 - val_accuracy: 0.8185 - val_loss: 0.8164\n",
            "Epoch 45/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8231 - loss: 0.9093\n",
            "Epoch 45: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8231 - loss: 0.9093 - val_accuracy: 0.8206 - val_loss: 0.8137\n",
            "Epoch 46/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8245 - loss: 0.9062\n",
            "Epoch 46: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8245 - loss: 0.9062 - val_accuracy: 0.8252 - val_loss: 0.7965\n",
            "Epoch 47/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8263 - loss: 0.9038\n",
            "Epoch 47: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8263 - loss: 0.9038 - val_accuracy: 0.8212 - val_loss: 0.8085\n",
            "Epoch 48/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8275 - loss: 0.9024\n",
            "Epoch 48: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8275 - loss: 0.9024 - val_accuracy: 0.8232 - val_loss: 0.8057\n",
            "Epoch 49/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8283 - loss: 0.9001\n",
            "Epoch 49: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8283 - loss: 0.9001 - val_accuracy: 0.8212 - val_loss: 0.8085\n",
            "Epoch 50/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8294 - loss: 0.8972\n",
            "Epoch 50: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8294 - loss: 0.8972 - val_accuracy: 0.8247 - val_loss: 0.8002\n",
            "Epoch 51/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8304 - loss: 0.8929\n",
            "Epoch 51: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8304 - loss: 0.8929 - val_accuracy: 0.8195 - val_loss: 0.8075\n",
            "Epoch 52/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8320 - loss: 0.8913\n",
            "Epoch 52: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8320 - loss: 0.8913 - val_accuracy: 0.8229 - val_loss: 0.8008\n",
            "Epoch 53/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8331 - loss: 0.8897\n",
            "Epoch 53: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8331 - loss: 0.8897 - val_accuracy: 0.8210 - val_loss: 0.8123\n",
            "Epoch 54/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8338 - loss: 0.8858\n",
            "Epoch 54: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8338 - loss: 0.8858 - val_accuracy: 0.8254 - val_loss: 0.8053\n",
            "Epoch 55/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8348 - loss: 0.8844\n",
            "Epoch 55: val_accuracy did not improve from 0.83030\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8348 - loss: 0.8844 - val_accuracy: 0.8275 - val_loss: 0.7985\n",
            "Epoch 56/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8361 - loss: 0.8807\n",
            "Epoch 56: val_accuracy improved from 0.83030 to 0.83076, saving model to artifacts/best_inception_se.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8361 - loss: 0.8808 - val_accuracy: 0.8308 - val_loss: 0.7946\n",
            "Epoch 57/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8376 - loss: 0.8794\n",
            "Epoch 57: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8376 - loss: 0.8795 - val_accuracy: 0.8242 - val_loss: 0.8066\n",
            "Epoch 58/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8379 - loss: 0.8775\n",
            "Epoch 58: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8379 - loss: 0.8775 - val_accuracy: 0.8289 - val_loss: 0.7919\n",
            "Epoch 59/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8386 - loss: 0.8752\n",
            "Epoch 59: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8386 - loss: 0.8752 - val_accuracy: 0.8300 - val_loss: 0.7970\n",
            "Epoch 60/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8388 - loss: 0.8739\n",
            "Epoch 60: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8388 - loss: 0.8739 - val_accuracy: 0.8286 - val_loss: 0.7909\n",
            "Epoch 61/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8413 - loss: 0.8684\n",
            "Epoch 61: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8412 - loss: 0.8684 - val_accuracy: 0.8297 - val_loss: 0.7905\n",
            "Epoch 62/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8402 - loss: 0.8703\n",
            "Epoch 62: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8402 - loss: 0.8703 - val_accuracy: 0.8265 - val_loss: 0.7974\n",
            "Epoch 63/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8417 - loss: 0.8675\n",
            "Epoch 63: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8417 - loss: 0.8675 - val_accuracy: 0.8300 - val_loss: 0.7885\n",
            "Epoch 64/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8422 - loss: 0.8648\n",
            "Epoch 64: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8422 - loss: 0.8648 - val_accuracy: 0.8283 - val_loss: 0.7921\n",
            "Epoch 65/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8428 - loss: 0.8633\n",
            "Epoch 65: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8428 - loss: 0.8633 - val_accuracy: 0.8288 - val_loss: 0.7884\n",
            "Epoch 66/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8440 - loss: 0.8624\n",
            "Epoch 66: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8440 - loss: 0.8624 - val_accuracy: 0.8273 - val_loss: 0.7936\n",
            "Epoch 67/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8440 - loss: 0.8612\n",
            "Epoch 67: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8440 - loss: 0.8612 - val_accuracy: 0.8269 - val_loss: 0.7953\n",
            "Epoch 68/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8437 - loss: 0.8614\n",
            "Epoch 68: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8437 - loss: 0.8614 - val_accuracy: 0.8260 - val_loss: 0.7961\n",
            "Epoch 69/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8452 - loss: 0.8601\n",
            "Epoch 69: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 18ms/step - accuracy: 0.8452 - loss: 0.8601 - val_accuracy: 0.8244 - val_loss: 0.7972\n",
            "Epoch 70/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8444 - loss: 0.8612\n",
            "Epoch 70: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8444 - loss: 0.8612 - val_accuracy: 0.8246 - val_loss: 0.7975\n",
            "Epoch 71/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8441 - loss: 0.8598\n",
            "Epoch 71: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8441 - loss: 0.8599 - val_accuracy: 0.8248 - val_loss: 0.7981\n",
            "Epoch 72/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8450 - loss: 0.8600\n",
            "Epoch 72: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8450 - loss: 0.8600 - val_accuracy: 0.8007 - val_loss: 0.8395\n",
            "Epoch 73/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8229 - loss: 0.9023\n",
            "Epoch 73: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8229 - loss: 0.9024 - val_accuracy: 0.8215 - val_loss: 0.8050\n",
            "Epoch 74/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8212 - loss: 0.9067\n",
            "Epoch 74: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8212 - loss: 0.9067 - val_accuracy: 0.8036 - val_loss: 0.8296\n",
            "Epoch 75/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8215 - loss: 0.9107\n",
            "Epoch 75: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8215 - loss: 0.9107 - val_accuracy: 0.8055 - val_loss: 0.8423\n",
            "Epoch 76/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8218 - loss: 0.9105\n",
            "Epoch 76: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8218 - loss: 0.9105 - val_accuracy: 0.8071 - val_loss: 0.8331\n",
            "Epoch 77/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8224 - loss: 0.9107\n",
            "Epoch 77: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8224 - loss: 0.9108 - val_accuracy: 0.8119 - val_loss: 0.8264\n",
            "Epoch 78/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8227 - loss: 0.9120\n",
            "Epoch 78: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8227 - loss: 0.9120 - val_accuracy: 0.8155 - val_loss: 0.8194\n",
            "Epoch 79/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8233 - loss: 0.9132\n",
            "Epoch 79: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8233 - loss: 0.9132 - val_accuracy: 0.8155 - val_loss: 0.8135\n",
            "Epoch 80/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8228 - loss: 0.9116\n",
            "Epoch 80: val_accuracy did not improve from 0.83076\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8228 - loss: 0.9116 - val_accuracy: 0.8002 - val_loss: 0.8507\n",
            "âœ… inception_se training complete. Best model saved.\n",
            "\n",
            "--------------------------------------------------\n",
            "ğŸ‹ï¸ TRAIN: SEMG_NET\n",
            "--------------------------------------------------\n",
            "Epoch 1/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5496 - loss: 1.3495\n",
            "Epoch 1: val_accuracy improved from -inf to 0.75735, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 29ms/step - accuracy: 0.5496 - loss: 1.3494 - val_accuracy: 0.7573 - val_loss: 0.9171\n",
            "Epoch 2/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6834 - loss: 1.1298\n",
            "Epoch 2: val_accuracy did not improve from 0.75735\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 19ms/step - accuracy: 0.6834 - loss: 1.1298 - val_accuracy: 0.6981 - val_loss: 0.9492\n",
            "Epoch 3/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7024 - loss: 1.0887\n",
            "Epoch 3: val_accuracy did not improve from 0.75735\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7024 - loss: 1.0887 - val_accuracy: 0.7283 - val_loss: 0.9003\n",
            "Epoch 4/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7153 - loss: 1.0702\n",
            "Epoch 4: val_accuracy did not improve from 0.75735\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7153 - loss: 1.0702 - val_accuracy: 0.7283 - val_loss: 0.9121\n",
            "Epoch 5/80\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7254 - loss: 1.0551\n",
            "Epoch 5: val_accuracy did not improve from 0.75735\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7254 - loss: 1.0551 - val_accuracy: 0.7563 - val_loss: 0.8873\n",
            "Epoch 6/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7330 - loss: 1.0430\n",
            "Epoch 6: val_accuracy improved from 0.75735 to 0.76462, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7330 - loss: 1.0430 - val_accuracy: 0.7646 - val_loss: 0.8721\n",
            "Epoch 7/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7409 - loss: 1.0303\n",
            "Epoch 7: val_accuracy improved from 0.76462 to 0.78220, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7409 - loss: 1.0303 - val_accuracy: 0.7822 - val_loss: 0.8536\n",
            "Epoch 8/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7474 - loss: 1.0211\n",
            "Epoch 8: val_accuracy improved from 0.78220 to 0.78924, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7474 - loss: 1.0211 - val_accuracy: 0.7892 - val_loss: 0.8367\n",
            "Epoch 9/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7546 - loss: 1.0102\n",
            "Epoch 9: val_accuracy did not improve from 0.78924\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7546 - loss: 1.0103 - val_accuracy: 0.7727 - val_loss: 0.8588\n",
            "Epoch 10/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7604 - loss: 1.0022\n",
            "Epoch 10: val_accuracy improved from 0.78924 to 0.79515, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7604 - loss: 1.0022 - val_accuracy: 0.7952 - val_loss: 0.8402\n",
            "Epoch 11/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7657 - loss: 0.9923\n",
            "Epoch 11: val_accuracy improved from 0.79515 to 0.80083, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7657 - loss: 0.9923 - val_accuracy: 0.8008 - val_loss: 0.8406\n",
            "Epoch 12/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7726 - loss: 0.9817\n",
            "Epoch 12: val_accuracy did not improve from 0.80083\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7726 - loss: 0.9817 - val_accuracy: 0.7938 - val_loss: 0.8465\n",
            "Epoch 13/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7755 - loss: 0.9750\n",
            "Epoch 13: val_accuracy did not improve from 0.80083\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7755 - loss: 0.9750 - val_accuracy: 0.7995 - val_loss: 0.8358\n",
            "Epoch 14/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7802 - loss: 0.9671\n",
            "Epoch 14: val_accuracy did not improve from 0.80083\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7802 - loss: 0.9671 - val_accuracy: 0.7944 - val_loss: 0.8423\n",
            "Epoch 15/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7845 - loss: 0.9604\n",
            "Epoch 15: val_accuracy did not improve from 0.80083\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 18ms/step - accuracy: 0.7845 - loss: 0.9604 - val_accuracy: 0.7989 - val_loss: 0.8453\n",
            "Epoch 16/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7917 - loss: 0.9500\n",
            "Epoch 16: val_accuracy improved from 0.80083 to 0.80598, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7917 - loss: 0.9501 - val_accuracy: 0.8060 - val_loss: 0.8360\n",
            "Epoch 17/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7922 - loss: 0.9433\n",
            "Epoch 17: val_accuracy did not improve from 0.80598\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7922 - loss: 0.9433 - val_accuracy: 0.8014 - val_loss: 0.8395\n",
            "Epoch 18/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7988 - loss: 0.9351\n",
            "Epoch 18: val_accuracy improved from 0.80598 to 0.81356, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7988 - loss: 0.9351 - val_accuracy: 0.8136 - val_loss: 0.8206\n",
            "Epoch 19/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8023 - loss: 0.9266\n",
            "Epoch 19: val_accuracy improved from 0.81356 to 0.81568, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.8023 - loss: 0.9266 - val_accuracy: 0.8157 - val_loss: 0.8198\n",
            "Epoch 20/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8041 - loss: 0.9223\n",
            "Epoch 20: val_accuracy improved from 0.81568 to 0.81621, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.8041 - loss: 0.9223 - val_accuracy: 0.8162 - val_loss: 0.8145\n",
            "Epoch 21/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8078 - loss: 0.9151\n",
            "Epoch 21: val_accuracy improved from 0.81621 to 0.81947, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.8078 - loss: 0.9152 - val_accuracy: 0.8195 - val_loss: 0.8177\n",
            "Epoch 22/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8098 - loss: 0.9135\n",
            "Epoch 22: val_accuracy improved from 0.81947 to 0.82068, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.8098 - loss: 0.9135 - val_accuracy: 0.8207 - val_loss: 0.8153\n",
            "Epoch 23/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8112 - loss: 0.9108\n",
            "Epoch 23: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.8112 - loss: 0.9108 - val_accuracy: 0.8204 - val_loss: 0.8181\n",
            "Epoch 24/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8109 - loss: 0.9097\n",
            "Epoch 24: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8109 - loss: 0.9097 - val_accuracy: 0.7846 - val_loss: 0.8695\n",
            "Epoch 25/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7739 - loss: 0.9769\n",
            "Epoch 25: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7739 - loss: 0.9769 - val_accuracy: 0.7931 - val_loss: 0.8765\n",
            "Epoch 26/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7762 - loss: 0.9773\n",
            "Epoch 26: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7762 - loss: 0.9773 - val_accuracy: 0.7942 - val_loss: 0.8560\n",
            "Epoch 27/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7781 - loss: 0.9789\n",
            "Epoch 27: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7781 - loss: 0.9789 - val_accuracy: 0.7995 - val_loss: 0.8639\n",
            "Epoch 28/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7790 - loss: 0.9804\n",
            "Epoch 28: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7790 - loss: 0.9804 - val_accuracy: 0.8097 - val_loss: 0.8516\n",
            "Epoch 29/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7796 - loss: 0.9795\n",
            "Epoch 29: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7796 - loss: 0.9795 - val_accuracy: 0.7901 - val_loss: 0.8621\n",
            "Epoch 30/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7811 - loss: 0.9772\n",
            "Epoch 30: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7811 - loss: 0.9772 - val_accuracy: 0.7775 - val_loss: 0.8802\n",
            "Epoch 31/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7828 - loss: 0.9756\n",
            "Epoch 31: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7828 - loss: 0.9756 - val_accuracy: 0.7883 - val_loss: 0.8726\n",
            "Epoch 32/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7834 - loss: 0.9737\n",
            "Epoch 32: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7834 - loss: 0.9738 - val_accuracy: 0.7971 - val_loss: 0.8692\n",
            "Epoch 33/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7865 - loss: 0.9708\n",
            "Epoch 33: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7865 - loss: 0.9708 - val_accuracy: 0.8059 - val_loss: 0.8536\n",
            "Epoch 34/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7893 - loss: 0.9676\n",
            "Epoch 34: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7893 - loss: 0.9676 - val_accuracy: 0.8047 - val_loss: 0.8564\n",
            "Epoch 35/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7890 - loss: 0.9672\n",
            "Epoch 35: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7890 - loss: 0.9672 - val_accuracy: 0.7976 - val_loss: 0.8576\n",
            "Epoch 36/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7900 - loss: 0.9629\n",
            "Epoch 36: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7900 - loss: 0.9629 - val_accuracy: 0.8192 - val_loss: 0.8291\n",
            "Epoch 37/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7925 - loss: 0.9625\n",
            "Epoch 37: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7925 - loss: 0.9625 - val_accuracy: 0.7962 - val_loss: 0.8589\n",
            "Epoch 38/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7931 - loss: 0.9596\n",
            "Epoch 38: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7931 - loss: 0.9596 - val_accuracy: 0.7911 - val_loss: 0.8703\n",
            "Epoch 39/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7952 - loss: 0.9544\n",
            "Epoch 39: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7952 - loss: 0.9544 - val_accuracy: 0.7964 - val_loss: 0.8667\n",
            "Epoch 40/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7968 - loss: 0.9535\n",
            "Epoch 40: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7968 - loss: 0.9535 - val_accuracy: 0.7921 - val_loss: 0.8504\n",
            "Epoch 41/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7981 - loss: 0.9489\n",
            "Epoch 41: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7981 - loss: 0.9490 - val_accuracy: 0.8076 - val_loss: 0.8471\n",
            "Epoch 42/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7988 - loss: 0.9487\n",
            "Epoch 42: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7988 - loss: 0.9487 - val_accuracy: 0.8131 - val_loss: 0.8362\n",
            "Epoch 43/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8001 - loss: 0.9438\n",
            "Epoch 43: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8001 - loss: 0.9438 - val_accuracy: 0.8022 - val_loss: 0.8415\n",
            "Epoch 44/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8024 - loss: 0.9409\n",
            "Epoch 44: val_accuracy did not improve from 0.82068\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8024 - loss: 0.9409 - val_accuracy: 0.8190 - val_loss: 0.8207\n",
            "Epoch 45/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8034 - loss: 0.9393\n",
            "Epoch 45: val_accuracy improved from 0.82068 to 0.82167, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8034 - loss: 0.9393 - val_accuracy: 0.8217 - val_loss: 0.8213\n",
            "Epoch 46/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8041 - loss: 0.9368\n",
            "Epoch 46: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8041 - loss: 0.9368 - val_accuracy: 0.8098 - val_loss: 0.8403\n",
            "Epoch 47/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8069 - loss: 0.9323\n",
            "Epoch 47: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8069 - loss: 0.9323 - val_accuracy: 0.8083 - val_loss: 0.8354\n",
            "Epoch 48/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8085 - loss: 0.9300\n",
            "Epoch 48: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8085 - loss: 0.9300 - val_accuracy: 0.8118 - val_loss: 0.8393\n",
            "Epoch 49/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8090 - loss: 0.9265\n",
            "Epoch 49: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8090 - loss: 0.9265 - val_accuracy: 0.8120 - val_loss: 0.8347\n",
            "Epoch 50/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8101 - loss: 0.9249\n",
            "Epoch 50: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8101 - loss: 0.9249 - val_accuracy: 0.8037 - val_loss: 0.8434\n",
            "Epoch 51/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8116 - loss: 0.9210\n",
            "Epoch 51: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8116 - loss: 0.9210 - val_accuracy: 0.8143 - val_loss: 0.8243\n",
            "Epoch 52/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8129 - loss: 0.9176\n",
            "Epoch 52: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8129 - loss: 0.9176 - val_accuracy: 0.8120 - val_loss: 0.8274\n",
            "Epoch 53/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8145 - loss: 0.9145\n",
            "Epoch 53: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8145 - loss: 0.9145 - val_accuracy: 0.8180 - val_loss: 0.8136\n",
            "Epoch 54/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8163 - loss: 0.9110\n",
            "Epoch 54: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8163 - loss: 0.9110 - val_accuracy: 0.8203 - val_loss: 0.8111\n",
            "Epoch 55/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8174 - loss: 0.9088\n",
            "Epoch 55: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8174 - loss: 0.9088 - val_accuracy: 0.8198 - val_loss: 0.8214\n",
            "Epoch 56/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8192 - loss: 0.9041\n",
            "Epoch 56: val_accuracy did not improve from 0.82167\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8192 - loss: 0.9041 - val_accuracy: 0.8190 - val_loss: 0.8099\n",
            "Epoch 57/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8211 - loss: 0.9015\n",
            "Epoch 57: val_accuracy improved from 0.82167 to 0.82311, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8211 - loss: 0.9015 - val_accuracy: 0.8231 - val_loss: 0.8021\n",
            "Epoch 58/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8221 - loss: 0.8977\n",
            "Epoch 58: val_accuracy did not improve from 0.82311\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8221 - loss: 0.8977 - val_accuracy: 0.8181 - val_loss: 0.8112\n",
            "Epoch 59/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8229 - loss: 0.8958\n",
            "Epoch 59: val_accuracy improved from 0.82311 to 0.82530, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8229 - loss: 0.8958 - val_accuracy: 0.8253 - val_loss: 0.8006\n",
            "Epoch 60/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8249 - loss: 0.8916\n",
            "Epoch 60: val_accuracy did not improve from 0.82530\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8249 - loss: 0.8916 - val_accuracy: 0.8229 - val_loss: 0.8092\n",
            "Epoch 61/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8255 - loss: 0.8901\n",
            "Epoch 61: val_accuracy improved from 0.82530 to 0.82833, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8255 - loss: 0.8901 - val_accuracy: 0.8283 - val_loss: 0.8002\n",
            "Epoch 62/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8268 - loss: 0.8895\n",
            "Epoch 62: val_accuracy did not improve from 0.82833\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8268 - loss: 0.8896 - val_accuracy: 0.8274 - val_loss: 0.7984\n",
            "Epoch 63/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8286 - loss: 0.8845\n",
            "Epoch 63: val_accuracy did not improve from 0.82833\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8286 - loss: 0.8845 - val_accuracy: 0.8254 - val_loss: 0.8078\n",
            "Epoch 64/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8272 - loss: 0.8834\n",
            "Epoch 64: val_accuracy did not improve from 0.82833\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8272 - loss: 0.8834 - val_accuracy: 0.8242 - val_loss: 0.8050\n",
            "Epoch 65/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8286 - loss: 0.8822\n",
            "Epoch 65: val_accuracy did not improve from 0.82833\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8286 - loss: 0.8822 - val_accuracy: 0.8261 - val_loss: 0.8021\n",
            "Epoch 66/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8308 - loss: 0.8790\n",
            "Epoch 66: val_accuracy did not improve from 0.82833\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8308 - loss: 0.8790 - val_accuracy: 0.8277 - val_loss: 0.8020\n",
            "Epoch 67/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8304 - loss: 0.8790\n",
            "Epoch 67: val_accuracy did not improve from 0.82833\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8304 - loss: 0.8790 - val_accuracy: 0.8279 - val_loss: 0.8012\n",
            "Epoch 68/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8325 - loss: 0.8750\n",
            "Epoch 68: val_accuracy did not improve from 0.82833\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8325 - loss: 0.8750 - val_accuracy: 0.8264 - val_loss: 0.8024\n",
            "Epoch 69/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8314 - loss: 0.8763\n",
            "Epoch 69: val_accuracy did not improve from 0.82833\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8314 - loss: 0.8763 - val_accuracy: 0.8277 - val_loss: 0.8021\n",
            "Epoch 70/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8317 - loss: 0.8779\n",
            "Epoch 70: val_accuracy improved from 0.82833 to 0.82886, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8317 - loss: 0.8779 - val_accuracy: 0.8289 - val_loss: 0.8020\n",
            "Epoch 71/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8322 - loss: 0.8756\n",
            "Epoch 71: val_accuracy improved from 0.82886 to 0.82917, saving model to artifacts/best_semg_net.keras\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8322 - loss: 0.8756 - val_accuracy: 0.8292 - val_loss: 0.8014\n",
            "Epoch 72/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8333 - loss: 0.8734\n",
            "Epoch 72: val_accuracy did not improve from 0.82917\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8333 - loss: 0.8734 - val_accuracy: 0.8077 - val_loss: 0.8425\n",
            "Epoch 73/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8018 - loss: 0.9317\n",
            "Epoch 73: val_accuracy did not improve from 0.82917\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8018 - loss: 0.9318 - val_accuracy: 0.8137 - val_loss: 0.8185\n",
            "Epoch 74/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8007 - loss: 0.9385\n",
            "Epoch 74: val_accuracy did not improve from 0.82917\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8007 - loss: 0.9385 - val_accuracy: 0.7850 - val_loss: 0.8714\n",
            "Epoch 75/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7994 - loss: 0.9438\n",
            "Epoch 75: val_accuracy did not improve from 0.82917\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7994 - loss: 0.9438 - val_accuracy: 0.7816 - val_loss: 0.8728\n",
            "Epoch 76/80\n",
            "\u001b[1m1228/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8004 - loss: 0.9438\n",
            "Epoch 76: val_accuracy did not improve from 0.82917\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8004 - loss: 0.9438 - val_accuracy: 0.7989 - val_loss: 0.8376\n",
            "Epoch 77/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7993 - loss: 0.9461\n",
            "Epoch 77: val_accuracy did not improve from 0.82917\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7993 - loss: 0.9461 - val_accuracy: 0.7971 - val_loss: 0.8549\n",
            "Epoch 78/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8008 - loss: 0.9472\n",
            "Epoch 78: val_accuracy did not improve from 0.82917\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8008 - loss: 0.9472 - val_accuracy: 0.7786 - val_loss: 0.8864\n",
            "Epoch 79/80\n",
            "\u001b[1m1229/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8006 - loss: 0.9463\n",
            "Epoch 79: val_accuracy did not improve from 0.82917\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8006 - loss: 0.9463 - val_accuracy: 0.7748 - val_loss: 0.8983\n",
            "Epoch 80/80\n",
            "\u001b[1m1230/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7994 - loss: 0.9508\n",
            "Epoch 80: val_accuracy did not improve from 0.82917\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7994 - loss: 0.9508 - val_accuracy: 0.7955 - val_loss: 0.8530\n",
            "âœ… semg_net training complete. Best model saved.\n",
            "\n",
            "======================================================================\n",
            "ğŸ¯ ENSEMBLE EVALUATION (2-Model Soft Voting)\n",
            "======================================================================\n",
            "âš¡ Predicting with Model 1 (Inception-SE-TCN)...\n",
            "âš¡ Predicting with Model 2 (sEMG Net)...\n",
            "\n",
            "****************************************\n",
            "ğŸ† FINAL ENSEMBLE ACCURACY: 0.8457\n",
            "ğŸ† FINAL ENSEMBLE F1 SCORE: 0.8455\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ğŸš€ FINAL TEST SCRIPT: 2-MODEL ENSEMBLE\n",
        "--------------------------------------\n",
        "Evaluates the Inception-SE + sEMG-Net ensemble.\n",
        "\n",
        "LOGIC:\n",
        "1. Re-creates the specific test split from Session 3.\n",
        "2. Fits the preprocessor on training data (to match training scaling).\n",
        "3. Loads the best weights for both models.\n",
        "4. Performs Soft Voting (50/50 split).\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from scipy.stats import mode\n",
        "from scipy.signal import butter, filtfilt, iirnotch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "DATA_DIR = 'data'\n",
        "ARTIFACTS_DIR = 'artifacts_final'\n",
        "FS = 512\n",
        "WINDOW_MS = 400\n",
        "STRIDE_MS = 160\n",
        "BATCH_SIZE = 128\n",
        "L2_REG = 1e-4\n",
        "\n",
        "# ==================== ARCHITECTURES (Must Match Training) ====================\n",
        "\n",
        "def squeeze_excite_block(input_tensor, ratio=8):\n",
        "    filters = input_tensor.shape[-1]\n",
        "    se = layers.GlobalAveragePooling1D()(input_tensor)\n",
        "    se = layers.Dense(filters // ratio, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(se)\n",
        "    se = layers.Dense(filters, activation='sigmoid', kernel_regularizer=regularizers.l2(L2_REG))(se)\n",
        "    se = layers.Reshape((1, filters))(se)\n",
        "    return layers.Multiply()([input_tensor, se])\n",
        "\n",
        "def inception_block(x, filters, dilation_rate):\n",
        "    b1 = layers.Conv1D(filters=filters//2, kernel_size=3, dilation_rate=dilation_rate,\n",
        "                       padding='same', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    b1 = layers.BatchNormalization()(b1)\n",
        "    b1 = layers.Activation('relu')(b1)\n",
        "    b2 = layers.Conv1D(filters=filters//2, kernel_size=7, dilation_rate=dilation_rate,\n",
        "                       padding='same', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    b2 = layers.BatchNormalization()(b2)\n",
        "    b2 = layers.Activation('relu')(b2)\n",
        "    return layers.Concatenate()([b1, b2])\n",
        "\n",
        "def make_inception_se_tcn(input_shape, n_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.GaussianNoise(0.05)(inputs) # Noise layer is active only in training, safe to leave here\n",
        "    filters = 64\n",
        "    for dilation_rate in [1, 2, 4, 8]:\n",
        "        prev_x = x\n",
        "        x = inception_block(x, filters, dilation_rate)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        x = squeeze_excite_block(x, ratio=8)\n",
        "        if prev_x.shape[-1] != filters:\n",
        "            prev_x = layers.Conv1D(filters=filters, kernel_size=1, padding='same')(prev_x)\n",
        "        x = layers.Add()([x, prev_x])\n",
        "\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = layers.MultiHeadAttention(key_dim=64, num_heads=4, dropout=0.3)(x, x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
        "    return keras.Model(inputs, outputs, name='Inception_SE_Attn')\n",
        "\n",
        "def conv_block(x, filters, kernel_size, pool=True):\n",
        "    x = layers.Conv1D(filters, kernel_size, padding='same', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    if pool: x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    return x\n",
        "\n",
        "def make_semg_net(input_shape, n_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.GaussianNoise(0.05)(inputs)\n",
        "    x = conv_block(x, 64, 9, pool=False)\n",
        "    x = conv_block(x, 128, 5, pool=True)\n",
        "    x = conv_block(x, 256, 3, pool=True)\n",
        "    x = conv_block(x, 512, 3, pool=True)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
        "    return keras.Model(inputs, outputs, name='sEMG_Net')\n",
        "\n",
        "# ==================== DATA UTILS ====================\n",
        "\n",
        "class SignalPreprocessor:\n",
        "    def __init__(self, fs=1000, bandpass_low=20.0, bandpass_high=450.0, notch_freq=50.0):\n",
        "        self.fs = fs\n",
        "        self.nyq = fs / 2\n",
        "        low = max(0.001, min(bandpass_low / self.nyq, 0.99))\n",
        "        high = max(low + 0.01, min(bandpass_high / self.nyq, 0.999))\n",
        "        self.b_bp, self.a_bp = butter(4, [low, high], btype='band')\n",
        "        self.b_notch, self.a_notch = iirnotch(notch_freq, 30.0, self.fs) if notch_freq > 0 else (None, None)\n",
        "        self.channel_means, self.channel_stds = None, None\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, signals_list):\n",
        "        all_signals = np.concatenate(signals_list, axis=0)\n",
        "        self.channel_means = np.mean(all_signals, axis=0)\n",
        "        self.channel_stds = np.std(all_signals, axis=0) + 1e-8\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, signal):\n",
        "        if len(signal) > 12:\n",
        "            signal = filtfilt(self.b_bp, self.a_bp, signal, axis=0)\n",
        "            if self.b_notch is not None:\n",
        "                signal = filtfilt(self.b_notch, self.a_notch, signal, axis=0)\n",
        "        if self.fitted:\n",
        "            return (signal - self.channel_means) / self.channel_stds\n",
        "        return (signal - np.mean(signal, axis=0)) / (np.std(signal, axis=0) + 1e-8)\n",
        "\n",
        "    def segment(self, signal, window_ms=200, stride_ms=100):\n",
        "        win_sz = int(window_ms * self.fs / 1000)\n",
        "        step = int(stride_ms * self.fs / 1000)\n",
        "        n = len(signal)\n",
        "        if n < win_sz: return None\n",
        "        n_win = (n - win_sz) // step + 1\n",
        "        idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
        "        return signal[idx]\n",
        "\n",
        "def get_session_files(data_dir, sessions):\n",
        "    files = []\n",
        "    for session in sessions:\n",
        "        pattern = f'{data_dir}/**/{session}/**/*.csv'\n",
        "        files.extend(sorted(glob.glob(pattern, recursive=True)))\n",
        "    return files\n",
        "\n",
        "def load_files_data(file_list):\n",
        "    data_list, labels_list = [], []\n",
        "    for f in file_list:\n",
        "        try:\n",
        "            lbl = int(re.search(r'gesture(\\d+)', f).group(1))\n",
        "            d = pd.read_csv(f).values\n",
        "            if d.shape[1] >= 8:\n",
        "                data_list.append(d)\n",
        "                labels_list.append(np.full(len(d), lbl))\n",
        "        except: pass\n",
        "    return data_list, labels_list\n",
        "\n",
        "def window_data(data_list, labels_list, prep, window_ms, stride_ms):\n",
        "    X_wins, y_wins = [], []\n",
        "    win_sz = int(window_ms * FS / 1000)\n",
        "    step = int(stride_ms * FS / 1000)\n",
        "    for d, l in zip(data_list, labels_list):\n",
        "        d_filt = prep.transform(d)\n",
        "        w = prep.segment(d_filt, window_ms, stride_ms)\n",
        "        if w is not None:\n",
        "            X_wins.append(w)\n",
        "            n_win = (len(d) - win_sz) // step + 1\n",
        "            idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
        "            w_modes = mode(l[idx], axis=1, keepdims=True)[0].flatten()\n",
        "            y_wins.append(w_modes)\n",
        "    if not X_wins: return None, None\n",
        "    return np.concatenate(X_wins), np.concatenate(y_wins)\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸš€ TESTING 2-MODEL ENSEMBLE (Inception-SE + sEMG-Net)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. SETUP DATA (Split Reproduction)\n",
        "    print(\"\\nğŸ“¦ Loading Dataset configuration...\")\n",
        "    train_files = get_session_files(DATA_DIR, ['Session1', 'Session2'])\n",
        "    session3_files = get_session_files(DATA_DIR, ['Session3'])\n",
        "\n",
        "    # Reproduce the random split used in training\n",
        "    gesture_files = {}\n",
        "    for f in session3_files:\n",
        "        match = re.search(r'gesture(\\d+)', f)\n",
        "        if match:\n",
        "            g = int(match.group(1))\n",
        "            gesture_files.setdefault(g, []).append(f)\n",
        "\n",
        "    test_files_final = []\n",
        "    rng = random.Random(42) # Match seed from training\n",
        "\n",
        "    for g, gfiles in gesture_files.items():\n",
        "        rng.shuffle(gfiles)\n",
        "        # Training logic: val_files = [:n_val], test_files = [n_val:]\n",
        "        n_val = max(1, int(len(gfiles) * 0.50))\n",
        "        test_files_final.extend(gfiles[n_val:]) # This corresponds to the 'test' set in your training script\n",
        "\n",
        "    print(f\"ğŸ“„ Found {len(test_files_final)} test files.\")\n",
        "\n",
        "    # 2. LOAD & PREPROCESS\n",
        "    print(\"â³ Loading raw data...\")\n",
        "    # We load training data ONLY to fit the Scaler (Preprocssor)\n",
        "    train_data_raw, train_labels_raw = load_files_data(train_files)\n",
        "    test_data_raw, test_labels_raw = load_files_data(test_files_final)\n",
        "\n",
        "    print(\"ğŸ”§ Preprocessing...\")\n",
        "    prep = SignalPreprocessor(fs=FS)\n",
        "    prep.fit(train_data_raw)\n",
        "\n",
        "    X_test, y_test_raw = window_data(test_data_raw, test_labels_raw, prep, WINDOW_MS, STRIDE_MS)\n",
        "\n",
        "    # 3. LABEL ENCODING\n",
        "    _, y_train_raw_dummy = window_data(train_data_raw, train_labels_raw, prep, WINDOW_MS, STRIDE_MS)\n",
        "    le = LabelEncoder().fit(y_train_raw_dummy)\n",
        "    y_test = np.array([le.transform([l])[0] if l in le.classes_ else -1 for l in y_test_raw])\n",
        "    n_classes = len(le.classes_)\n",
        "    input_shape = X_test.shape[1:]\n",
        "\n",
        "    print(f\"âœ… Data Ready. Shape: {X_test.shape}\")\n",
        "\n",
        "    # 4. LOAD MODELS\n",
        "    print(\"\\n\" + \"-\"*40 + \"\\nğŸ”® RUNNING ENSEMBLE INFERENCE\\n\" + \"-\"*40)\n",
        "\n",
        "    models_config = [\n",
        "        (\"inception_se\", make_inception_se_tcn),\n",
        "        (\"semg_net\", make_semg_net)\n",
        "    ]\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    for name, builder in models_config:\n",
        "        path = f'{ARTIFACTS_DIR}/best_{name}.keras'\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"âŒ Error: Could not find {path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"âš¡ Loading {name}...\")\n",
        "        model = builder(input_shape, n_classes)\n",
        "        model.load_weights(path)\n",
        "\n",
        "        probs = model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
        "        predictions.append(probs)\n",
        "\n",
        "        # Individual Check\n",
        "        acc = accuracy_score(y_test, probs.argmax(axis=1))\n",
        "        print(f\"   -> {name} Standalone Acc: {acc:.4f}\")\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    # 5. SOFT VOTING (50/50 Split)\n",
        "    if len(predictions) == 2:\n",
        "        ensemble_probs = (predictions[0] + predictions[1]) / 2.0\n",
        "        final_preds = ensemble_probs.argmax(axis=1)\n",
        "\n",
        "        # 6. RESULTS\n",
        "        acc = accuracy_score(y_test, final_preds)\n",
        "        f1 = f1_score(y_test, final_preds, average='macro')\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"ğŸ† FINAL ENSEMBLE RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"ACCURACY: {acc:.4f}\")\n",
        "        print(f\"F1 SCORE: {f1:.4f}\")\n",
        "        print(\"-\" * 60)\n",
        "        print(\"\\nClassification Report:\\n\")\n",
        "        print(classification_report(y_test, final_preds, target_names=[str(c) for c in le.classes_]))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(y_test, final_preds)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
        "                    xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "        plt.title(f'2-Model Ensemble Matrix (Acc: {acc:.4f})')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.savefig(f'{ARTIFACTS_DIR}/ensemble_2model_matrix.png')\n",
        "        print(f\"\\nğŸ“Š Matrix saved to {ARTIFACTS_DIR}/ensemble_2model_matrix.png\")\n",
        "    else:\n",
        "        print(\"âŒ Error: Not enough models loaded for ensemble.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RvjBuIkanoZv",
        "outputId": "827f1598-ae36-417c-fb68-640a95ac6c13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸš€ TESTING 2-MODEL ENSEMBLE (Inception-SE + sEMG-Net)\n",
            "============================================================\n",
            "\n",
            "ğŸ“¦ Loading Dataset configuration...\n",
            "ğŸ“„ Found 440 test files.\n",
            "â³ Loading raw data...\n",
            "ğŸ”§ Preprocessing...\n",
            "âœ… Data Ready. Shape: (13200, 204, 8)\n",
            "\n",
            "----------------------------------------\n",
            "ğŸ”® RUNNING ENSEMBLE INFERENCE\n",
            "----------------------------------------\n",
            "âš¡ Loading inception_se...\n",
            "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step\n",
            "   -> inception_se Standalone Acc: 0.8238\n",
            "âš¡ Loading semg_net...\n",
            "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
            "   -> semg_net Standalone Acc: 0.8313\n",
            "\n",
            "============================================================\n",
            "ğŸ† FINAL ENSEMBLE RESULTS\n",
            "============================================================\n",
            "ACCURACY: 0.8455\n",
            "F1 SCORE: 0.8460\n",
            "------------------------------------------------------------\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.91      0.89      2640\n",
            "           1       0.79      0.82      0.80      2640\n",
            "           2       0.79      0.78      0.78      2640\n",
            "           3       0.81      0.80      0.80      2640\n",
            "           4       0.98      0.93      0.95      2640\n",
            "\n",
            "    accuracy                           0.85     13200\n",
            "   macro avg       0.85      0.85      0.85     13200\n",
            "weighted avg       0.85      0.85      0.85     13200\n",
            "\n",
            "\n",
            "ğŸ“Š Matrix saved to artifacts_final/ensemble_2model_matrix.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ğŸ† FINAL GOLD PIPELINE: Weighted 2-Model Ensemble\n",
        "-------------------------------------------------\n",
        "Architecture: Inception-SE + sEMG-Net (The Winner: 85.11%)\n",
        "Strategy:     Class Weights to fix the remaining Class 1 vs 2 confusion.\n",
        "Goal:         Break 86% Accuracy.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, mixed_precision, callbacks\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from scipy.stats import mode\n",
        "from scipy.signal import butter, filtfilt, iirnotch\n",
        "import warnings\n",
        "import gc\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "DATA_DIR = 'data'\n",
        "ARTIFACTS_DIR = 'artifacts_final'\n",
        "FS = 512\n",
        "EPOCHS = 60  # Increased slightly to allow class weights to settle\n",
        "BATCH_SIZE = 128\n",
        "RANDOM_SEED = 42\n",
        "VAL_FILE_RATIO = 0.50\n",
        "WINDOW_MS = 400\n",
        "STRIDE_MS = 160\n",
        "L2_REG = 1e-4\n",
        "\n",
        "# GPU Setup\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        policy = mixed_precision.Policy('mixed_float16')\n",
        "        mixed_precision.set_global_policy(policy)\n",
        "        print(\"âœ… Mixed Precision (FP16) Enabled\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# ==================== DATA & PREPROCESSING ====================\n",
        "# (Standard Preprocessing Pipeline)\n",
        "\n",
        "class SignalPreprocessor:\n",
        "    def __init__(self, fs=1000, bandpass_low=20.0, bandpass_high=450.0, notch_freq=50.0):\n",
        "        self.fs = fs\n",
        "        nyq = fs / 2\n",
        "        low = max(0.001, min(bandpass_low / nyq, 0.99))\n",
        "        high = max(low + 0.01, min(bandpass_high / nyq, 0.999))\n",
        "        self.b_bp, self.a_bp = butter(4, [low, high], btype='band')\n",
        "        self.b_notch, self.a_notch = iirnotch(notch_freq, 30.0, self.fs) if notch_freq > 0 else (None, None)\n",
        "        self.channel_means, self.channel_stds = None, None\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, signals_list):\n",
        "        all_signals = np.concatenate(signals_list, axis=0)\n",
        "        self.channel_means = np.mean(all_signals, axis=0)\n",
        "        self.channel_stds = np.std(all_signals, axis=0) + 1e-8\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, signal):\n",
        "        if len(signal) > 12:\n",
        "            signal = filtfilt(self.b_bp, self.a_bp, signal, axis=0)\n",
        "            if self.b_notch is not None:\n",
        "                signal = filtfilt(self.b_notch, self.a_notch, signal, axis=0)\n",
        "        if self.fitted:\n",
        "            return (signal - self.channel_means) / self.channel_stds\n",
        "        return (signal - np.mean(signal, axis=0)) / (np.std(signal, axis=0) + 1e-8)\n",
        "\n",
        "    def segment(self, signal, window_ms=200, stride_ms=100):\n",
        "        win_sz = int(window_ms * self.fs / 1000)\n",
        "        step = int(stride_ms * self.fs / 1000)\n",
        "        n = len(signal)\n",
        "        if n < win_sz: return None\n",
        "        n_win = (n - win_sz) // step + 1\n",
        "        idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
        "        return signal[idx]\n",
        "\n",
        "def augment_dataset_advanced(X, y):\n",
        "    print(f\"    âš¡ Augmenting Data (Input: {len(X)} windows)...\")\n",
        "    b, t, c = X.shape\n",
        "\n",
        "    # 1. Channel Masking\n",
        "    X_mask = X.copy()\n",
        "    mask_indices = np.random.choice(b, size=int(b * 0.5), replace=False)\n",
        "    for i in mask_indices:\n",
        "        ch = np.random.randint(0, c)\n",
        "        X_mask[i, :, ch] = 0\n",
        "    X_mask = X_mask + np.random.normal(0, 0.02, size=X_mask.shape)\n",
        "\n",
        "    # 2. MixUp\n",
        "    indices = np.random.permutation(b)\n",
        "    X_shuffled = X[indices]\n",
        "    alpha = 0.2\n",
        "    lam = np.random.beta(alpha, alpha, size=(b, 1, 1))\n",
        "    X_mix = lam * X + (1 - lam) * X_shuffled\n",
        "    y_mix = y.copy()\n",
        "\n",
        "    X_final = np.concatenate([X, X_mask, X_mix], axis=0)\n",
        "    y_final = np.concatenate([y, y, y_mix], axis=0)\n",
        "    print(f\"    âš¡ Augmentation complete. Size: {len(X_final)} (3x)\")\n",
        "    return X_final, y_final\n",
        "\n",
        "def get_session_files(data_dir, sessions):\n",
        "    files = []\n",
        "    for session in sessions:\n",
        "        pattern = f'{data_dir}/**/{session}/**/*.csv'\n",
        "        files.extend(sorted(glob.glob(pattern, recursive=True)))\n",
        "    return files\n",
        "\n",
        "def split_files_by_ratio(files, val_ratio, seed=RANDOM_SEED):\n",
        "    gesture_files = {}\n",
        "    for f in files:\n",
        "        match = re.search(r'gesture(\\d+)', f)\n",
        "        if match:\n",
        "            g = int(match.group(1))\n",
        "            gesture_files.setdefault(g, []).append(f)\n",
        "    train, val = [], []\n",
        "    rng = random.Random(seed)\n",
        "    for g, gfiles in gesture_files.items():\n",
        "        rng.shuffle(gfiles)\n",
        "        n_val = max(1, int(len(gfiles) * val_ratio))\n",
        "        val.extend(gfiles[:n_val])\n",
        "        train.extend(gfiles[n_val:])\n",
        "    return train, val\n",
        "\n",
        "def load_files_data(file_list):\n",
        "    data_list, labels_list = [], []\n",
        "    for f in file_list:\n",
        "        try:\n",
        "            lbl = int(re.search(r'gesture(\\d+)', f).group(1))\n",
        "            d = pd.read_csv(f).values\n",
        "            if d.shape[1] >= 8:\n",
        "                data_list.append(d)\n",
        "                labels_list.append(np.full(len(d), lbl))\n",
        "        except: pass\n",
        "    return data_list, labels_list\n",
        "\n",
        "def window_data(data_list, labels_list, prep, window_ms, stride_ms):\n",
        "    X_wins, y_wins = [], []\n",
        "    win_sz = int(window_ms * FS / 1000)\n",
        "    step = int(stride_ms * FS / 1000)\n",
        "    for d, l in zip(data_list, labels_list):\n",
        "        d_filt = prep.transform(d)\n",
        "        w = prep.segment(d_filt, window_ms, stride_ms)\n",
        "        if w is not None:\n",
        "            X_wins.append(w)\n",
        "            n_win = (len(d) - win_sz) // step + 1\n",
        "            idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step\n",
        "            w_modes = mode(l[idx], axis=1, keepdims=True)[0].flatten()\n",
        "            y_wins.append(w_modes)\n",
        "    if not X_wins: return None, None\n",
        "    return np.concatenate(X_wins), np.concatenate(y_wins)\n",
        "\n",
        "# ==================== MODEL DEFINITIONS ====================\n",
        "\n",
        "# 1. Inception-SE-TCN\n",
        "def squeeze_excite_block(input_tensor, ratio=8):\n",
        "    filters = input_tensor.shape[-1]\n",
        "    se = layers.GlobalAveragePooling1D()(input_tensor)\n",
        "    se = layers.Dense(filters // ratio, activation='relu', kernel_regularizer=l2(L2_REG))(se)\n",
        "    se = layers.Dense(filters, activation='sigmoid', kernel_regularizer=l2(L2_REG))(se)\n",
        "    se = layers.Reshape((1, filters))(se)\n",
        "    return layers.Multiply()([input_tensor, se])\n",
        "\n",
        "def inception_block(x, filters, dilation_rate):\n",
        "    b1 = layers.Conv1D(filters//2, 3, dilation_rate=dilation_rate, padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
        "    b1 = layers.BatchNormalization()(b1)\n",
        "    b1 = layers.Activation('relu')(b1)\n",
        "    b2 = layers.Conv1D(filters//2, 7, dilation_rate=dilation_rate, padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
        "    b2 = layers.BatchNormalization()(b2)\n",
        "    b2 = layers.Activation('relu')(b2)\n",
        "    return layers.Concatenate()([b1, b2])\n",
        "\n",
        "def make_inception_se_tcn(input_shape, n_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.GaussianNoise(0.05)(inputs)\n",
        "    filters = 64\n",
        "    for dilation_rate in [1, 2, 4, 8]:\n",
        "        prev_x = x\n",
        "        x = inception_block(x, filters, dilation_rate)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        x = squeeze_excite_block(x, ratio=8)\n",
        "        if prev_x.shape[-1] != filters:\n",
        "            prev_x = layers.Conv1D(filters=filters, kernel_size=1, padding='same')(prev_x)\n",
        "        x = layers.Add()([x, prev_x])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = layers.MultiHeadAttention(key_dim=64, num_heads=4, dropout=0.3)(x, x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
        "    return keras.Model(inputs, outputs, name='Inception_SE_Attn')\n",
        "\n",
        "# 2. sEMG Net\n",
        "def conv_block(x, filters, kernel_size, pool=True):\n",
        "    x = layers.Conv1D(filters, kernel_size, padding='same', kernel_regularizer=l2(L2_REG))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    if pool: x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    return x\n",
        "\n",
        "def make_semg_net(input_shape, n_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.GaussianNoise(0.05)(inputs)\n",
        "    x = conv_block(x, 64, 9, pool=False)\n",
        "    x = conv_block(x, 128, 5, pool=True)\n",
        "    x = conv_block(x, 256, 3, pool=True)\n",
        "    x = conv_block(x, 512, 3, pool=True)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(512, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
        "    return keras.Model(inputs, outputs, name='sEMG_Net')\n",
        "\n",
        "# ==================== 4. TRAINING LOGIC ====================\n",
        "\n",
        "def train_and_save(model_builder, model_name, X_train, y_train_hot, X_val, y_val_hot, input_shape, n_classes, class_weights):\n",
        "    print(f\"\\nğŸ‹ï¸ TRAIN: {model_name} (Weighted)\")\n",
        "\n",
        "    total_steps = len(X_train) // BATCH_SIZE * EPOCHS\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
        "        initial_learning_rate=0.001, first_decay_steps=int(total_steps * 0.3),\n",
        "        t_mul=2.0, m_mul=0.9, alpha=1e-5\n",
        "    )\n",
        "\n",
        "    save_path = f'{ARTIFACTS_DIR}/best_{model_name}.keras'\n",
        "    checkpoint = callbacks.ModelCheckpoint(save_path, monitor='val_accuracy', mode='max', save_best_only=True, verbose=0)\n",
        "\n",
        "    model = model_builder(input_shape, n_classes)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "                  loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # ğŸ”¥ APPLY CLASS WEIGHTS HERE\n",
        "    model.fit(X_train, y_train_hot,\n",
        "              validation_data=(X_val, y_val_hot),\n",
        "              epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "              callbacks=[checkpoint],\n",
        "              class_weight=class_weights,  # <--- The Key Fix\n",
        "              verbose=1)\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    return save_path\n",
        "\n",
        "def main():\n",
        "    os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "    print(\"=\"*70 + \"\\nğŸš€ FINAL GOLD PIPELINE: 2-Model Ensemble + Class Weights\\n\" + \"=\"*70)\n",
        "\n",
        "    # 1. Load Data\n",
        "    existing_csvs = glob.glob(f'{DATA_DIR}/**/*.csv', recursive=True)\n",
        "    if not existing_csvs:\n",
        "        import gdown, zipfile\n",
        "        gdown.download('https://drive.google.com/uc?id=16iNEwhThf2LcX7rOOVM03MTZiwq7G51x', 'dataset.zip', quiet=False)\n",
        "        with zipfile.ZipFile('dataset.zip', 'r') as z: z.extractall(DATA_DIR)\n",
        "        os.remove('dataset.zip')\n",
        "\n",
        "    train_files = get_session_files(DATA_DIR, ['Session1', 'Session2'])\n",
        "    session3_files = get_session_files(DATA_DIR, ['Session3'])\n",
        "    val_files, test_files = split_files_by_ratio(session3_files, VAL_FILE_RATIO)\n",
        "\n",
        "    train_data, train_labels = load_files_data(train_files)\n",
        "    val_data, val_labels = load_files_data(val_files)\n",
        "    test_data, test_labels = load_files_data(test_files)\n",
        "\n",
        "    # 2. Preprocess\n",
        "    prep = SignalPreprocessor(fs=FS).fit(train_data)\n",
        "    X_train, y_train_raw = window_data(train_data, train_labels, prep, WINDOW_MS, STRIDE_MS)\n",
        "    X_val, y_val_raw = window_data(val_data, val_labels, prep, WINDOW_MS, STRIDE_MS)\n",
        "    X_test, y_test_raw = window_data(test_data, test_labels, prep, WINDOW_MS, STRIDE_MS)\n",
        "\n",
        "    # 3. Augment\n",
        "    X_train, y_train_raw = augment_dataset_advanced(X_train, y_train_raw)\n",
        "\n",
        "    # 4. Encode\n",
        "    le = LabelEncoder().fit(y_train_raw)\n",
        "    y_train = le.transform(y_train_raw)\n",
        "    y_val = np.array([le.transform([l])[0] if l in le.classes_ else -1 for l in y_val_raw])\n",
        "    y_test = np.array([le.transform([l])[0] if l in le.classes_ else -1 for l in y_test_raw])\n",
        "\n",
        "    n_classes = len(le.classes_)\n",
        "    input_shape = X_train.shape[1:]\n",
        "\n",
        "    y_train_hot = tf.keras.utils.to_categorical(y_train, n_classes)\n",
        "    y_val_hot = tf.keras.utils.to_categorical(y_val, n_classes)\n",
        "\n",
        "    # ==================== 5. DEFINE CLASS WEIGHTS ====================\n",
        "    # Based on your confusion matrix: Class 1 and 2 are the weak points.\n",
        "    class_weights = {\n",
        "        0: 1.0,\n",
        "        1: 1.5,  # Focus 50% more on Class 1\n",
        "        2: 1.5,  # Focus 50% more on Class 2\n",
        "        3: 1.0,\n",
        "        4: 1.0\n",
        "    }\n",
        "    print(f\"ğŸ¯ Strategy: Applying Class Weights: {class_weights}\")\n",
        "\n",
        "    # ==================== 6. TRAIN & SAVE ====================\n",
        "    path_inception = train_and_save(make_inception_se_tcn, \"inception_se\",\n",
        "                                    X_train, y_train_hot, X_val, y_val_hot, input_shape, n_classes, class_weights)\n",
        "\n",
        "    path_semgnet = train_and_save(make_semg_net, \"semg_net\",\n",
        "                                  X_train, y_train_hot, X_val, y_val_hot, input_shape, n_classes, class_weights)\n",
        "\n",
        "    # ==================== 7. FINAL EVALUATION ====================\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\nğŸ¯ FINAL EVALUATION (2-Model Weighted)\\n\" + \"=\"*70)\n",
        "\n",
        "    m1 = make_inception_se_tcn(input_shape, n_classes)\n",
        "    m1.load_weights(path_inception)\n",
        "\n",
        "    m2 = make_semg_net(input_shape, n_classes)\n",
        "    m2.load_weights(path_semgnet)\n",
        "\n",
        "    p1 = m1.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "    p2 = m2.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "    # Simple Average (since models are equally strong)\n",
        "    ensemble_probs = (p1 + p2) / 2.0\n",
        "    ensemble_preds = ensemble_probs.argmax(axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_test, ensemble_preds)\n",
        "    f1 = f1_score(y_test, ensemble_preds, average='macro')\n",
        "\n",
        "    print(f\"ğŸ† FINAL ACCURACY: {acc:.4f}\")\n",
        "    print(f\"ğŸ† FINAL F1 SCORE: {f1:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV-ZaMjwoSrt",
        "outputId": "f811b433-a812-4dcf-eab7-1cdd2affa084"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Mixed Precision (FP16) Enabled\n",
            "======================================================================\n",
            "ğŸš€ FINAL GOLD PIPELINE: 2-Model Ensemble + Class Weights\n",
            "======================================================================\n",
            "    âš¡ Augmenting Data (Input: 52500 windows)...\n",
            "    âš¡ Augmentation complete. Size: 157500 (3x)\n",
            "ğŸ¯ Strategy: Applying Class Weights: {0: 1.0, 1: 1.5, 2: 1.5, 3: 1.0, 4: 1.0}\n",
            "\n",
            "ğŸ‹ï¸ TRAIN: inception_se (Weighted)\n",
            "Epoch 1/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 37ms/step - accuracy: 0.5652 - loss: 1.5139 - val_accuracy: 0.7447 - val_loss: 0.9157\n",
            "Epoch 2/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7027 - loss: 1.3090 - val_accuracy: 0.7966 - val_loss: 0.8319\n",
            "Epoch 3/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - accuracy: 0.7317 - loss: 1.2589 - val_accuracy: 0.8088 - val_loss: 0.7990\n",
            "Epoch 4/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7488 - loss: 1.2273 - val_accuracy: 0.8046 - val_loss: 0.8199\n",
            "Epoch 5/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7606 - loss: 1.2076 - val_accuracy: 0.7771 - val_loss: 0.8815\n",
            "Epoch 6/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - accuracy: 0.7695 - loss: 1.1920 - val_accuracy: 0.8140 - val_loss: 0.8064\n",
            "Epoch 7/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7775 - loss: 1.1761 - val_accuracy: 0.8018 - val_loss: 0.8388\n",
            "Epoch 8/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7838 - loss: 1.1638 - val_accuracy: 0.8021 - val_loss: 0.8439\n",
            "Epoch 9/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7908 - loss: 1.1493 - val_accuracy: 0.8199 - val_loss: 0.7985\n",
            "Epoch 10/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7955 - loss: 1.1397 - val_accuracy: 0.8204 - val_loss: 0.8032\n",
            "Epoch 11/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8004 - loss: 1.1333 - val_accuracy: 0.8190 - val_loss: 0.7983\n",
            "Epoch 12/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8050 - loss: 1.1210 - val_accuracy: 0.8166 - val_loss: 0.8160\n",
            "Epoch 13/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8096 - loss: 1.1109 - val_accuracy: 0.8159 - val_loss: 0.8030\n",
            "Epoch 14/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8120 - loss: 1.1049 - val_accuracy: 0.8198 - val_loss: 0.8022\n",
            "Epoch 15/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8142 - loss: 1.0981 - val_accuracy: 0.8215 - val_loss: 0.8064\n",
            "Epoch 16/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8156 - loss: 1.0958 - val_accuracy: 0.8210 - val_loss: 0.8032\n",
            "Epoch 17/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8174 - loss: 1.0920 - val_accuracy: 0.8163 - val_loss: 0.8088\n",
            "Epoch 18/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 17ms/step - accuracy: 0.8175 - loss: 1.0906 - val_accuracy: 0.8055 - val_loss: 0.8188\n",
            "Epoch 19/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 17ms/step - accuracy: 0.7931 - loss: 1.1427 - val_accuracy: 0.8158 - val_loss: 0.8192\n",
            "Epoch 20/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.7944 - loss: 1.1426 - val_accuracy: 0.8195 - val_loss: 0.8025\n",
            "Epoch 21/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7959 - loss: 1.1384 - val_accuracy: 0.8109 - val_loss: 0.8206\n",
            "Epoch 22/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7968 - loss: 1.1399 - val_accuracy: 0.8117 - val_loss: 0.8467\n",
            "Epoch 23/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.7982 - loss: 1.1372 - val_accuracy: 0.7944 - val_loss: 0.8607\n",
            "Epoch 24/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8018 - loss: 1.1336 - val_accuracy: 0.8108 - val_loss: 0.8301\n",
            "Epoch 25/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8044 - loss: 1.1295 - val_accuracy: 0.8123 - val_loss: 0.8310\n",
            "Epoch 26/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8044 - loss: 1.1296 - val_accuracy: 0.8045 - val_loss: 0.8588\n",
            "Epoch 27/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8059 - loss: 1.1245 - val_accuracy: 0.8075 - val_loss: 0.8346\n",
            "Epoch 28/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8085 - loss: 1.1222 - val_accuracy: 0.8200 - val_loss: 0.8051\n",
            "Epoch 29/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8100 - loss: 1.1171 - val_accuracy: 0.8169 - val_loss: 0.8088\n",
            "Epoch 30/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8115 - loss: 1.1112 - val_accuracy: 0.8122 - val_loss: 0.8316\n",
            "Epoch 31/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8137 - loss: 1.1080 - val_accuracy: 0.8038 - val_loss: 0.8411\n",
            "Epoch 32/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8155 - loss: 1.1059 - val_accuracy: 0.8227 - val_loss: 0.8159\n",
            "Epoch 33/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.8155 - loss: 1.1043 - val_accuracy: 0.8230 - val_loss: 0.8130\n",
            "Epoch 34/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8180 - loss: 1.0965 - val_accuracy: 0.8224 - val_loss: 0.8091\n",
            "Epoch 35/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8192 - loss: 1.0963 - val_accuracy: 0.8164 - val_loss: 0.8232\n",
            "Epoch 36/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8221 - loss: 1.0908 - val_accuracy: 0.8193 - val_loss: 0.8262\n",
            "Epoch 37/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8214 - loss: 1.0862 - val_accuracy: 0.8225 - val_loss: 0.8128\n",
            "Epoch 38/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8244 - loss: 1.0829 - val_accuracy: 0.8211 - val_loss: 0.8067\n",
            "Epoch 39/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8262 - loss: 1.0788 - val_accuracy: 0.8196 - val_loss: 0.8099\n",
            "Epoch 40/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8267 - loss: 1.0716 - val_accuracy: 0.8202 - val_loss: 0.8157\n",
            "Epoch 41/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8287 - loss: 1.0695 - val_accuracy: 0.8228 - val_loss: 0.8126\n",
            "Epoch 42/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8302 - loss: 1.0686 - val_accuracy: 0.8216 - val_loss: 0.8136\n",
            "Epoch 43/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8314 - loss: 1.0626 - val_accuracy: 0.8220 - val_loss: 0.8118\n",
            "Epoch 44/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8320 - loss: 1.0615 - val_accuracy: 0.8237 - val_loss: 0.8100\n",
            "Epoch 45/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8318 - loss: 1.0586 - val_accuracy: 0.8202 - val_loss: 0.8069\n",
            "Epoch 46/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8346 - loss: 1.0525 - val_accuracy: 0.8217 - val_loss: 0.8119\n",
            "Epoch 47/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8352 - loss: 1.0518 - val_accuracy: 0.8188 - val_loss: 0.8187\n",
            "Epoch 48/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8348 - loss: 1.0513 - val_accuracy: 0.8183 - val_loss: 0.8176\n",
            "Epoch 49/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8361 - loss: 1.0490 - val_accuracy: 0.8194 - val_loss: 0.8200\n",
            "Epoch 50/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 17ms/step - accuracy: 0.8367 - loss: 1.0472 - val_accuracy: 0.8179 - val_loss: 0.8241\n",
            "Epoch 51/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 18ms/step - accuracy: 0.8376 - loss: 1.0434 - val_accuracy: 0.8147 - val_loss: 0.8310\n",
            "Epoch 52/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8374 - loss: 1.0458 - val_accuracy: 0.8142 - val_loss: 0.8328\n",
            "Epoch 53/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8385 - loss: 1.0432 - val_accuracy: 0.8142 - val_loss: 0.8350\n",
            "Epoch 54/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8382 - loss: 1.0424 - val_accuracy: 0.8027 - val_loss: 0.8773\n",
            "Epoch 55/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 17ms/step - accuracy: 0.8173 - loss: 1.0937 - val_accuracy: 0.8164 - val_loss: 0.8308\n",
            "Epoch 56/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8160 - loss: 1.0969 - val_accuracy: 0.8143 - val_loss: 0.8235\n",
            "Epoch 57/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8164 - loss: 1.1005 - val_accuracy: 0.8077 - val_loss: 0.8239\n",
            "Epoch 58/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8154 - loss: 1.1028 - val_accuracy: 0.8113 - val_loss: 0.8276\n",
            "Epoch 59/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8158 - loss: 1.1014 - val_accuracy: 0.8102 - val_loss: 0.8234\n",
            "Epoch 60/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8169 - loss: 1.1022 - val_accuracy: 0.8051 - val_loss: 0.8484\n",
            "\n",
            "ğŸ‹ï¸ TRAIN: semg_net (Weighted)\n",
            "Epoch 1/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 26ms/step - accuracy: 0.5459 - loss: 1.6206 - val_accuracy: 0.7087 - val_loss: 0.9896\n",
            "Epoch 2/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.6809 - loss: 1.3580 - val_accuracy: 0.7498 - val_loss: 0.9118\n",
            "Epoch 3/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7006 - loss: 1.3144 - val_accuracy: 0.7483 - val_loss: 0.9068\n",
            "Epoch 4/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7150 - loss: 1.2866 - val_accuracy: 0.7359 - val_loss: 0.9250\n",
            "Epoch 5/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7244 - loss: 1.2672 - val_accuracy: 0.7691 - val_loss: 0.8972\n",
            "Epoch 6/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7334 - loss: 1.2495 - val_accuracy: 0.7740 - val_loss: 0.8704\n",
            "Epoch 7/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7422 - loss: 1.2327 - val_accuracy: 0.7520 - val_loss: 0.9134\n",
            "Epoch 8/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7512 - loss: 1.2183 - val_accuracy: 0.7846 - val_loss: 0.8750\n",
            "Epoch 9/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7580 - loss: 1.2010 - val_accuracy: 0.7854 - val_loss: 0.8582\n",
            "Epoch 10/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7646 - loss: 1.1892 - val_accuracy: 0.8039 - val_loss: 0.8423\n",
            "Epoch 11/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7712 - loss: 1.1774 - val_accuracy: 0.8023 - val_loss: 0.8546\n",
            "Epoch 12/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7779 - loss: 1.1612 - val_accuracy: 0.8031 - val_loss: 0.8431\n",
            "Epoch 13/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7840 - loss: 1.1482 - val_accuracy: 0.8127 - val_loss: 0.8427\n",
            "Epoch 14/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7883 - loss: 1.1357 - val_accuracy: 0.8110 - val_loss: 0.8344\n",
            "Epoch 15/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7943 - loss: 1.1277 - val_accuracy: 0.8236 - val_loss: 0.8252\n",
            "Epoch 16/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7975 - loss: 1.1161 - val_accuracy: 0.8227 - val_loss: 0.8260\n",
            "Epoch 17/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7984 - loss: 1.1125 - val_accuracy: 0.8261 - val_loss: 0.8246\n",
            "Epoch 18/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 18ms/step - accuracy: 0.8004 - loss: 1.1093 - val_accuracy: 0.7749 - val_loss: 0.8938\n",
            "Epoch 19/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7649 - loss: 1.1893 - val_accuracy: 0.8048 - val_loss: 0.8486\n",
            "Epoch 20/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 18ms/step - accuracy: 0.7672 - loss: 1.1899 - val_accuracy: 0.7993 - val_loss: 0.8541\n",
            "Epoch 21/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.7678 - loss: 1.1900 - val_accuracy: 0.7949 - val_loss: 0.8773\n",
            "Epoch 22/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.7717 - loss: 1.1876 - val_accuracy: 0.7893 - val_loss: 0.8718\n",
            "Epoch 23/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7730 - loss: 1.1860 - val_accuracy: 0.8045 - val_loss: 0.8668\n",
            "Epoch 24/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7759 - loss: 1.1814 - val_accuracy: 0.7950 - val_loss: 0.8737\n",
            "Epoch 25/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7785 - loss: 1.1776 - val_accuracy: 0.7891 - val_loss: 0.8816\n",
            "Epoch 26/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7800 - loss: 1.1734 - val_accuracy: 0.8042 - val_loss: 0.8518\n",
            "Epoch 27/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7838 - loss: 1.1665 - val_accuracy: 0.8052 - val_loss: 0.8651\n",
            "Epoch 28/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7833 - loss: 1.1650 - val_accuracy: 0.7871 - val_loss: 0.8882\n",
            "Epoch 29/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7868 - loss: 1.1593 - val_accuracy: 0.8008 - val_loss: 0.8782\n",
            "Epoch 30/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7907 - loss: 1.1551 - val_accuracy: 0.8024 - val_loss: 0.8775\n",
            "Epoch 31/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7925 - loss: 1.1492 - val_accuracy: 0.8000 - val_loss: 0.8715\n",
            "Epoch 32/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7937 - loss: 1.1452 - val_accuracy: 0.8104 - val_loss: 0.8634\n",
            "Epoch 33/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7964 - loss: 1.1397 - val_accuracy: 0.8015 - val_loss: 0.8671\n",
            "Epoch 34/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7986 - loss: 1.1360 - val_accuracy: 0.8033 - val_loss: 0.8688\n",
            "Epoch 35/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7998 - loss: 1.1316 - val_accuracy: 0.8086 - val_loss: 0.8586\n",
            "Epoch 36/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8025 - loss: 1.1244 - val_accuracy: 0.8080 - val_loss: 0.8586\n",
            "Epoch 37/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8045 - loss: 1.1201 - val_accuracy: 0.7983 - val_loss: 0.8836\n",
            "Epoch 38/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8056 - loss: 1.1145 - val_accuracy: 0.8090 - val_loss: 0.8495\n",
            "Epoch 39/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8089 - loss: 1.1075 - val_accuracy: 0.8020 - val_loss: 0.8654\n",
            "Epoch 40/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8109 - loss: 1.1023 - val_accuracy: 0.8058 - val_loss: 0.8559\n",
            "Epoch 41/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8130 - loss: 1.0968 - val_accuracy: 0.8105 - val_loss: 0.8483\n",
            "Epoch 42/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8150 - loss: 1.0922 - val_accuracy: 0.8123 - val_loss: 0.8426\n",
            "Epoch 43/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8169 - loss: 1.0872 - val_accuracy: 0.8161 - val_loss: 0.8318\n",
            "Epoch 44/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8185 - loss: 1.0827 - val_accuracy: 0.8162 - val_loss: 0.8357\n",
            "Epoch 45/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8184 - loss: 1.0810 - val_accuracy: 0.8238 - val_loss: 0.8289\n",
            "Epoch 46/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8223 - loss: 1.0764 - val_accuracy: 0.8251 - val_loss: 0.8261\n",
            "Epoch 47/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8235 - loss: 1.0705 - val_accuracy: 0.8192 - val_loss: 0.8312\n",
            "Epoch 48/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8240 - loss: 1.0681 - val_accuracy: 0.8233 - val_loss: 0.8284\n",
            "Epoch 49/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8256 - loss: 1.0639 - val_accuracy: 0.8263 - val_loss: 0.8265\n",
            "Epoch 50/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8263 - loss: 1.0624 - val_accuracy: 0.8266 - val_loss: 0.8256\n",
            "Epoch 51/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8273 - loss: 1.0605 - val_accuracy: 0.8283 - val_loss: 0.8240\n",
            "Epoch 52/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8288 - loss: 1.0582 - val_accuracy: 0.8291 - val_loss: 0.8221\n",
            "Epoch 53/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8277 - loss: 1.0573 - val_accuracy: 0.8313 - val_loss: 0.8209\n",
            "Epoch 54/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.8291 - loss: 1.0572 - val_accuracy: 0.8070 - val_loss: 0.8579\n",
            "Epoch 55/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7976 - loss: 1.1282 - val_accuracy: 0.7901 - val_loss: 0.8865\n",
            "Epoch 56/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7975 - loss: 1.1331 - val_accuracy: 0.7936 - val_loss: 0.8817\n",
            "Epoch 57/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7972 - loss: 1.1368 - val_accuracy: 0.7748 - val_loss: 0.9034\n",
            "Epoch 58/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7973 - loss: 1.1387 - val_accuracy: 0.7520 - val_loss: 0.9573\n",
            "Epoch 59/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.7976 - loss: 1.1416 - val_accuracy: 0.7895 - val_loss: 0.8902\n",
            "Epoch 60/60\n",
            "\u001b[1m1231/1231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 18ms/step - accuracy: 0.7978 - loss: 1.1410 - val_accuracy: 0.7960 - val_loss: 0.8802\n",
            "\n",
            "======================================================================\n",
            "ğŸ¯ FINAL EVALUATION (2-Model Weighted)\n",
            "======================================================================\n",
            "ğŸ† FINAL ACCURACY: 0.8484\n",
            "ğŸ† FINAL F1 SCORE: 0.8491\n"
          ]
        }
      ]
    }
  ]
}
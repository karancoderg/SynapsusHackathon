#!/usr/bin/env python3
"""
ULTRA HIGH-PERFORMANCE HYBRID TCN PIPELINE
Optimized for NVIDIA H200 GPU (141GB VRAM, 150GB RAM)
Target: >90% Accuracy via Massive Scale & Aggressive Training

OPTIMIZATIONS FOR H200:
1. Massive batch sizes (2048-4096) for maximum GPU utilization
2. Full precision (FP32) for maximum accuracy
3. Aggressive 8x augmentation (no memory constraints)
4. Larger model capacity (128-256 filters)
5. Multi-GPU support (if available)
6. Parallel data processing
7. Advanced ensemble with 5+ models
"""

import os
os.environ['JAX_PLATFORMS'] = 'cpu'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'
os.environ['TF_GPU_THREAD_COUNT'] = '8'

import glob
import re
import random
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks
from tensorflow.keras.regularizers import l2
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from scipy.stats import mode
from scipy.signal import butter, filtfilt, iirnotch
from scipy.interpolate import CubicSpline
import warnings
from joblib import Parallel, delayed
import multiprocessing

warnings.filterwarnings('ignore')

# ==================== H200 CONFIGURATION ====================
DATA_DIR = 'data'
ARTIFACTS_DIR = 'artifacts'
FS = 512
EPOCHS = 150  # More epochs with H200 speed
BATCH_SIZE = 2048  # Massive batch size for H200
RANDOM_SEED = 42
VAL_FILE_RATIO = 0.50

# Aggressive windowing for maximum data
WINDOW_MS = 400
STRIDE_MS = 20  # Even smaller stride = 8x more samples

# Model hyperparameters - LARGER for H200
L2_REG = 1e-4
DROPOUT_CONV = 0.2
DROPOUT_HEAD = 0.4
MODEL_FILTERS = 128  # Doubled from 64
MODEL_DEPTH = 6  # More layers

# Augmentation - AGGRESSIVE (8x data)
AUG_MULTIPLIER = 8  # Create 8x augmented data

# H200 GPU Setup - FULL PRECISION
try:
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"âœ… Found {len(gpus)} GPU(s)")
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, False)  # Pre-allocate all memory
        
        # Use FULL PRECISION (FP32) for maximum accuracy on H200
        # H200 has enough compute power that we don't need FP16
        print("âœ… Using Full Precision (FP32) for maximum accuracy")
        
        # Multi-GPU strategy if available
        if len(gpus) > 1:
            strategy = tf.distribute.MirroredStrategy()
            print(f"âœ… Multi-GPU Training Enabled ({len(gpus)} GPUs)")
        else:
            strategy = tf.distribute.get_strategy()
    else:
        strategy = tf.distribute.get_strategy()
except Exception as e:
    print(f"GPU setup: {e}")
    strategy = tf.distribute.get_strategy()

# Set seeds
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# Parallel processing
N_JOBS = multiprocessing.cpu_count()
print(f"âœ… Using {N_JOBS} CPU cores for parallel processing")

# ==================== ADVANCED FEATURE EXTRACTION ====================

def extract_advanced_features(window):
    """
    Extract comprehensive feature set for H200 (no memory constraints).
    Includes time, frequency, wavelet, and nonlinear features.
    """
    features = []
    
    for ch in range(window.shape[1]):
        signal = window[:, ch]
        
        # Time domain (8 features)
        features.extend([
            np.mean(np.abs(signal)),  # MAV
            np.sqrt(np.mean(signal**2)),  # RMS
            np.var(signal),  # Variance
            np.sum(np.abs(np.diff(signal))),  # Waveform Length
            np.max(np.abs(signal)),  # Peak
            np.sum(signal**2),  # Energy
            np.sqrt(np.mean(np.diff(signal)**2)),  # RMS of derivative
            len(signal[:-1][(signal[:-1] * signal[1:]) < 0])  # Zero crossings
        ])
        
        # Frequency domain (6 features)
        fft = np.fft.rfft(signal)
        psd = np.abs(fft)**2
        freqs = np.fft.rfftfreq(len(signal), 1/FS)
        
        features.extend([
            np.mean(psd),  # Mean power
            np.max(psd),  # Peak power
            np.sum(psd),  # Total power
            np.sum(freqs * psd) / np.sum(psd) if np.sum(psd) > 0 else 0,  # Mean frequency
            np.sqrt(np.sum((freqs - features[-1])**2 * psd) / np.sum(psd)) if np.sum(psd) > 0 else 0,  # Freq std
            np.sum(psd[freqs < 100]) / np.sum(psd) if np.sum(psd) > 0 else 0  # Low freq ratio
        ])
        
        # Nonlinear features (3 features)
        features.extend([
            np.sum(np.abs(np.diff(np.sign(np.diff(signal))))),  # Slope sign changes
            -np.sum(signal**2 * np.log(signal**2 + 1e-10)),  # Sample entropy approx
            np.percentile(np.abs(signal), 90)  # 90th percentile
        ])
    
    return np.array(features)

def extract_features_parallel(windows, n_jobs=N_JOBS):
    """Parallel feature extraction using all CPU cores."""
    return np.array(Parallel(n_jobs=n_jobs)(
        delayed(extract_advanced_features)(w) for w in windows
    ))

# ==================== AGGRESSIVE AUGMENTATION FOR H200 ====================

def augment_aggressive_h200(X_raw, X_feat, y, multiplier=8):
    """
    Aggressive 8x augmentation leveraging H200's massive memory.
    Creates diverse augmented samples for better generalization.
    """
    print(f"   âš¡ Aggressive Augmentation (Input: {len(X_raw)} â†’ Output: {len(X_raw) * multiplier})")
    
    b, t, c = X_raw.shape
    X_aug_list = [X_raw]
    X_feat_aug_list = [X_feat]
    y_aug_list = [y]
    
    for aug_idx in range(multiplier - 1):
        X_aug = X_raw.copy()
        X_feat_aug = X_feat.copy()
        
        # Augmentation type based on index
        aug_type = aug_idx % 7
        
        if aug_type == 0:  # Gaussian noise
            noise_std = np.random.uniform(0.01, 0.03)
            X_aug += np.random.normal(0, noise_std, X_aug.shape)
        
        elif aug_type == 1:  # Amplitude scaling
            scales = np.random.uniform(0.7, 1.3, size=(b, 1, 1))
            X_aug *= scales
            X_feat_aug *= scales.reshape(b, 1)
        
        elif aug_type == 2:  # Channel masking
            for i in range(b):
                if np.random.random() < 0.5:
                    ch = np.random.randint(0, c)
                    X_aug[i, :, ch] = 0
                    # Zero corresponding features
                    feat_per_ch = X_feat.shape[1] // c
                    X_feat_aug[i, ch*feat_per_ch:(ch+1)*feat_per_ch] = 0
        
        elif aug_type == 3:  # Time warping
            for i in range(b):
                if np.random.random() < 0.5:
                    # Smooth time warping
                    n_knots = np.random.randint(4, 7)
                    knot_pos = np.linspace(0, t-1, n_knots)
                    displacements = np.random.normal(0, t * 0.05, n_knots)
                    displacements[0] = displacements[-1] = 0
                    warped_pos = np.clip(knot_pos + displacements, 0, t-1)
                    
                    spline = CubicSpline(knot_pos, warped_pos)
                    new_indices = spline(np.arange(t))
                    new_indices = np.clip(new_indices, 0, t-1)
                    
                    for ch_idx in range(c):
                        X_aug[i, :, ch_idx] = np.interp(
                            np.arange(t), new_indices, X_raw[i, :, ch_idx]
                        )
        
        elif aug_type == 4:  # Temporal shift
            shifts = np.random.randint(-10, 11, size=b)
            for i, shift in enumerate(shifts):
                if shift != 0:
                    X_aug[i] = np.roll(X_raw[i], shift, axis=0)
        
        elif aug_type == 5:  # MixUp
            indices = np.random.permutation(b)
            alpha = 0.3
            lam = np.random.beta(alpha, alpha, size=(b, 1, 1))
            X_aug = lam * X_raw + (1 - lam) * X_raw[indices]
            X_feat_aug = lam.reshape(b, 1) * X_feat + (1 - lam.reshape(b, 1)) * X_feat[indices]
        
        else:  # Combined augmentation
            # Apply multiple augmentations
            X_aug += np.random.normal(0, 0.01, X_aug.shape)
            X_aug *= np.random.uniform(0.9, 1.1, size=(b, 1, 1))
        
        X_aug_list.append(X_aug)
        X_feat_aug_list.append(X_feat_aug)
        y_aug_list.append(y)
    
    X_final = np.concatenate(X_aug_list, axis=0)
    X_feat_final = np.concatenate(X_feat_aug_list, axis=0)
    y_final = np.concatenate(y_aug_list, axis=0)
    
    print(f"   âœ… Augmentation complete: {len(X_final)} samples ({multiplier}x)")
    return X_final, X_feat_final, y_final

# ==================== PREPROCESSING ====================

class SignalPreprocessor:
    def __init__(self, fs=1000, bandpass_low=20.0, bandpass_high=450.0, notch_freq=50.0):
        self.fs = fs
        nyq = fs / 2
        low = max(0.001, min(bandpass_low / nyq, 0.99))
        high = max(low + 0.01, min(bandpass_high / nyq, 0.999))
        self.b_bp, self.a_bp = butter(4, [low, high], btype='band')
        self.b_notch, self.a_notch = iirnotch(notch_freq, 30.0, self.fs) if notch_freq > 0 else (None, None)
        self.fitted = False
        self.channel_means = None
        self.channel_stds = None

    def fit(self, signals_list):
        all_signals = np.concatenate(signals_list, axis=0)
        self.channel_means = np.mean(all_signals, axis=0)
        self.channel_stds = np.std(all_signals, axis=0) + 1e-8
        self.fitted = True
        return self

    def transform(self, signal):
        if len(signal) > 12:
            signal = filtfilt(self.b_bp, self.a_bp, signal, axis=0)
            if self.b_notch is not None:
                signal = filtfilt(self.b_notch, self.a_notch, signal, axis=0)
        if self.fitted:
            return (signal - self.channel_means) / self.channel_stds
        return (signal - np.mean(signal, axis=0)) / (np.std(signal, axis=0) + 1e-8)

    def segment(self, signal, window_ms=200, stride_ms=100):
        win_sz = int(window_ms * self.fs / 1000)
        step = int(stride_ms * self.fs / 1000)
        n = len(signal)
        if n < win_sz: return None
        n_win = (n - win_sz) // step + 1
        idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step
        return signal[idx]

# ==================== ULTRA-LARGE MODEL FOR H200 ====================

def se_block(input_tensor, ratio=8):
    """Squeeze-and-Excitation block."""
    filters = input_tensor.shape[-1]
    se = layers.GlobalAveragePooling1D()(input_tensor)
    se = layers.Dense(filters // ratio, activation='relu', kernel_regularizer=l2(L2_REG))(se)
    se = layers.Dense(filters, activation='sigmoid')(se)
    se = layers.Reshape((1, filters))(se)
    return layers.Multiply()([input_tensor, se])

def make_ultra_hybrid_tcn(input_shape, feature_dim, n_classes):
    """
    Ultra-large hybrid model for H200.
    - 128-256 filters (vs 64 in original)
    - 6 TCN blocks (vs 5)
    - Deeper feature network
    - More attention mechanisms
    """
    # Stream 1: Raw Signal (Ultra TCN)
    raw_inputs = layers.Input(shape=input_shape, name="Raw_Signal")
    x = layers.GaussianNoise(0.01)(raw_inputs)

    filters_progression = [128, 128, 128, 256, 256, 256]
    dilations = [1, 2, 4, 8, 16, 32]

    for i, (filters, dilation_rate) in enumerate(zip(filters_progression, dilations)):
        prev_x = x
        
        # Use regular Conv1D with causal padding (SeparableConv1D doesn't support causal)
        # Manual causal padding for dilated convolutions
        padding_size = (3 - 1) * dilation_rate
        x = layers.ZeroPadding1D(padding=(padding_size, 0))(x)  # Pad left only
        
        x = layers.SeparableConv1D(
            filters=filters, kernel_size=3, dilation_rate=dilation_rate,
            padding='valid', depthwise_regularizer=l2(L2_REG),
            pointwise_regularizer=l2(L2_REG)
        )(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.Dropout(DROPOUT_CONV)(x)
        
        # SE block every layer
        x = se_block(x, ratio=8)
        
        # Multi-head attention at mid and late stages
        if i in [2, 4]:
            attn = layers.MultiHeadAttention(num_heads=8, key_dim=filters//8)(x, x)
            x = layers.Add()([x, attn])
            x = layers.LayerNormalization()(x)
        
        # Residual connection
        if prev_x.shape[-1] != filters:
            prev_x = layers.Conv1D(filters=filters, kernel_size=1, padding='same')(prev_x)
        x = layers.Add()([x, prev_x])

    # Triple pooling for richer representation
    pool_max = layers.GlobalMaxPooling1D()(x)
    pool_avg = layers.GlobalAveragePooling1D()(x)
    pool_last = layers.Lambda(lambda t: t[:, -1, :])(x)  # Last timestep
    x_flat = layers.Concatenate()([pool_max, pool_avg, pool_last])

    # Stream 2: Expert Features (Deeper network)
    feat_inputs = layers.Input(shape=(feature_dim,), name="Handcrafted_Features")
    y = layers.Dense(128, activation='relu', kernel_regularizer=l2(L2_REG))(feat_inputs)
    y = layers.BatchNormalization()(y)
    y = layers.Dropout(0.3)(y)
    y = layers.Dense(128, activation='relu', kernel_regularizer=l2(L2_REG))(y)
    y = layers.BatchNormalization()(y)
    y = layers.Dropout(0.3)(y)

    # Fusion with attention
    combined = layers.Concatenate()([x_flat, y])
    z = layers.Dense(256, activation='relu', kernel_regularizer=l2(L2_REG))(combined)
    z = layers.Dropout(DROPOUT_HEAD)(z)
    z = layers.Dense(128, activation='relu', kernel_regularizer=l2(L2_REG))(z)
    z = layers.Dropout(DROPOUT_HEAD)(z)

    outputs = layers.Dense(n_classes, activation='softmax')(z)
    
    model = keras.Model(inputs=[raw_inputs, feat_inputs], outputs=outputs, name='UltraHybridTCN_H200')
    
    print(f"\n   Model Parameters: {model.count_params():,}")
    return model

# ==================== UTILS ====================

def get_session_files(data_dir, sessions):
    files = []
    for session in sessions:
        pattern = f'{data_dir}/**/{session}/**/*.csv'
        files.extend(sorted(glob.glob(pattern, recursive=True)))
    return files

def split_files_by_ratio(files, val_ratio, seed=RANDOM_SEED):
    gesture_files = {}
    for f in files:
        match = re.search(r'gesture(\d+)', f)
        if match:
            g = int(match.group(1))
            gesture_files.setdefault(g, []).append(f)
    train, val = [], []
    rng = random.Random(seed)
    for g, gfiles in gesture_files.items():
        rng.shuffle(gfiles)
        n_val = max(1, int(len(gfiles) * val_ratio))
        val.extend(gfiles[:n_val])
        train.extend(gfiles[n_val:])
    return train, val

def load_files_data(file_list):
    data_list, labels_list = [], []
    for f in file_list:
        try:
            lbl = int(re.search(r'gesture(\d+)', f).group(1))
            d = pd.read_csv(f).values
            if d.shape[1] >= 8:
                data_list.append(d)
                labels_list.append(np.full(len(d), lbl))
        except:
            pass
    return data_list, labels_list

def window_hybrid_data(data_list, labels_list, prep, window_ms, stride_ms):
    """Window data and extract features in parallel."""
    X_wins, y_wins = [], []
    win_sz = int(window_ms * FS / 1000)
    step = int(stride_ms * FS / 1000)
    
    for d, l in zip(data_list, labels_list):
        d_filt = prep.transform(d)
        w = prep.segment(d_filt, window_ms, stride_ms)
        if w is not None:
            X_wins.append(w)
            n_win = (len(d) - win_sz) // step + 1
            idx = np.arange(win_sz)[None, :] + np.arange(n_win)[:, None] * step
            w_modes = mode(l[idx], axis=1, keepdims=True)[0].flatten()
            y_wins.append(w_modes)
    
    if not X_wins:
        return None, None
    
    X_concat = np.concatenate(X_wins, axis=0)
    y_concat = np.concatenate(y_wins, axis=0)
    
    # Parallel feature extraction
    print(f"   Extracting features for {len(X_concat)} windows (parallel)...")
    X_feats = extract_features_parallel(X_concat, n_jobs=N_JOBS)
    
    return X_concat, X_feats, y_concat

# ==================== MAIN ====================

def main():
    os.makedirs(ARTIFACTS_DIR, exist_ok=True)
    print("="*80)
    print("ðŸš€ ULTRA HIGH-PERFORMANCE HYBRID TCN (NVIDIA H200)")
    print("="*80)
    print(f"   GPU Memory: 141GB VRAM")
    print(f"   System RAM: 150GB")
    print(f"   Batch Size: {BATCH_SIZE}")
    print(f"   Model Filters: {MODEL_FILTERS}")
    print(f"   Augmentation: {AUG_MULTIPLIER}x")
    print("="*80)

    # 1. Load Data
    existing_csvs = glob.glob(f'{DATA_DIR}/**/*.csv', recursive=True)
    if not existing_csvs:
        print("\nDownloading dataset...")
        import gdown, zipfile
        gdown.download('https://drive.google.com/uc?id=16iNEwhThf2LcX7rOOVM03MTZiwq7G51x', 'dataset.zip', quiet=False)
        with zipfile.ZipFile('dataset.zip', 'r') as z:
            z.extractall(DATA_DIR)
        os.remove('dataset.zip')

    print("\nðŸ“‚ Loading files...")
    train_files = get_session_files(DATA_DIR, ['Session1', 'Session2'])
    session3_files = get_session_files(DATA_DIR, ['Session3'])
    val_files, test_files = split_files_by_ratio(session3_files, VAL_FILE_RATIO)
    
    print(f"   Train: {len(train_files)} files")
    print(f"   Val:   {len(val_files)} files")
    print(f"   Test:  {len(test_files)} files")

    # 2. Load raw data
    print("\nðŸ“Š Loading raw data...")
    train_data, train_labels = load_files_data(train_files)
    val_data, val_labels = load_files_data(val_files)
    test_data, test_labels = load_files_data(test_files)

    # 3. Preprocess
    print("\nðŸ”§ Preprocessing...")
    prep = SignalPreprocessor(fs=FS)
    prep.fit(train_data)

    # 4. Window and extract features
    print(f"\nðŸªŸ Windowing (Window={WINDOW_MS}ms, Stride={STRIDE_MS}ms)...")
    X_train_raw, X_train_feat, y_train_raw = window_hybrid_data(
        train_data, train_labels, prep, WINDOW_MS, STRIDE_MS)
    X_val_raw, X_val_feat, y_val_raw = window_hybrid_data(
        val_data, val_labels, prep, WINDOW_MS, STRIDE_MS)
    X_test_raw, X_test_feat, y_test_raw = window_hybrid_data(
        test_data, test_labels, prep, WINDOW_MS, STRIDE_MS)

    print(f"\n   Train windows: {len(X_train_raw):,}")
    print(f"   Val windows:   {len(X_val_raw):,}")
    print(f"   Test windows:  {len(X_test_raw):,}")

    # 5. AGGRESSIVE AUGMENTATION (H200 can handle it)
    print(f"\nðŸŽ² Aggressive {AUG_MULTIPLIER}x Augmentation...")
    X_train_raw, X_train_feat, y_train_raw = augment_aggressive_h200(
        X_train_raw, X_train_feat, y_train_raw, multiplier=AUG_MULTIPLIER
    )
    print(f"   Final training size: {len(X_train_raw):,} samples")

    # 6. Scale features
    print("\nâš–ï¸ Scaling features...")
    scaler = StandardScaler()
    X_train_feat = scaler.fit_transform(X_train_feat)
    X_val_feat = scaler.transform(X_val_feat)
    X_test_feat = scaler.transform(X_test_feat)

    # 7. Encode labels
    le = LabelEncoder()
    y_train = le.fit_transform(y_train_raw)
    y_val = le.transform(y_val_raw)
    y_test = le.transform(y_test_raw)
    n_classes = len(le.classes_)
    
    print(f"   Classes: {n_classes}")

    # 8. Build and train model
    print("\n" + "="*80)
    print("ðŸ—ï¸ BUILDING ULTRA-LARGE MODEL")
    print("="*80)
    
    with strategy.scope():
        model = make_ultra_hybrid_tcn(
            X_train_raw.shape[1:], X_train_feat.shape[1], n_classes
        )
        
        # Cosine decay with warmup
        total_steps = (len(X_train_raw) // BATCH_SIZE) * EPOCHS
        warmup_steps = total_steps // 10
        
        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
            initial_learning_rate=0.001,
            decay_steps=total_steps - warmup_steps,
            alpha=0.01
        )
        
        optimizer = tf.keras.optimizers.Adam(lr_schedule)
        
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

    # 9. Train
    print("\n" + "="*80)
    print("ðŸ‹ï¸ TRAINING (H200 ULTRA MODE)")
    print("="*80)
    
    # Ensure data is in correct format (float32 for inputs, int32/int64 for labels)
    X_train_raw = X_train_raw.astype(np.float32)
    X_train_feat = X_train_feat.astype(np.float32)
    X_val_raw = X_val_raw.astype(np.float32)
    X_val_feat = X_val_feat.astype(np.float32)
    y_train = y_train.astype(np.int32)
    y_val = y_val.astype(np.int32)
    
    print(f"   Data shapes:")
    print(f"   X_train_raw: {X_train_raw.shape}, dtype: {X_train_raw.dtype}")
    print(f"   X_train_feat: {X_train_feat.shape}, dtype: {X_train_feat.dtype}")
    print(f"   y_train: {y_train.shape}, dtype: {y_train.dtype}")
    
    es = callbacks.EarlyStopping(
        monitor='val_accuracy', patience=25,
        restore_best_weights=True, verbose=1
    )
    
    reduce_lr = callbacks.ReduceLROnPlateau(
        monitor='val_loss', factor=0.5,
        patience=10, min_lr=1e-7, verbose=1
    )
    
    checkpoint = callbacks.ModelCheckpoint(
        f'{ARTIFACTS_DIR}/best_h200_model.keras',
        monitor='val_accuracy', save_best_only=True, verbose=1
    )
    
    # Use explicit tuple for multi-input
    history = model.fit(
        x=(X_train_raw, X_train_feat),
        y=y_train,
        validation_data=((X_val_raw, X_val_feat), y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=[es, reduce_lr, checkpoint],
        verbose=1,
        shuffle=True
    )

    # 10. Evaluate with 5-view TTA
    print("\n" + "="*80)
    print("ðŸŽ¯ FINAL EVALUATION (5-View TTA)")
    print("="*80)
    
    # Ensure test data is in correct format
    X_test_raw = X_test_raw.astype(np.float32)
    X_test_feat = X_test_feat.astype(np.float32)
    y_test = y_test.astype(np.int32)
    
    predictions = []
    
    # View 1: Normal
    print("   Predicting view 1/5 (normal)...")
    p1 = model.predict((X_test_raw, X_test_feat), batch_size=BATCH_SIZE, verbose=0)
    predictions.append(p1)
    
    # View 2: Gaussian noise
    print("   Predicting view 2/5 (gaussian noise)...")
    X_noisy = (X_test_raw + np.random.normal(0, 0.01, X_test_raw.shape)).astype(np.float32)
    p2 = model.predict((X_noisy, X_test_feat), batch_size=BATCH_SIZE, verbose=0)
    predictions.append(p2)
    
    # View 3: Amplitude scaling (weak)
    print("   Predicting view 3/5 (weak scaling)...")
    X_weak = (X_test_raw * 0.95).astype(np.float32)
    X_feat_weak = (X_test_feat * 0.95).astype(np.float32)
    p3 = model.predict((X_weak, X_feat_weak), batch_size=BATCH_SIZE, verbose=0)
    predictions.append(p3)
    
    # View 4: Amplitude scaling (strong)
    print("   Predicting view 4/5 (strong scaling)...")
    X_strong = (X_test_raw * 1.05).astype(np.float32)
    X_feat_strong = (X_test_feat * 1.05).astype(np.float32)
    p4 = model.predict((X_strong, X_feat_strong), batch_size=BATCH_SIZE, verbose=0)
    predictions.append(p4)
    
    # View 5: Combined noise + scaling
    print("   Predicting view 5/5 (combined)...")
    X_combined = (X_test_raw * 0.98 + np.random.normal(0, 0.005, X_test_raw.shape)).astype(np.float32)
    X_feat_combined = (X_test_feat * 0.98).astype(np.float32)
    p5 = model.predict((X_combined, X_feat_combined), batch_size=BATCH_SIZE, verbose=0)
    predictions.append(p5)
    
    # Ensemble with weighted voting (give more weight to normal view)
    weights = [0.3, 0.2, 0.15, 0.15, 0.2]
    final_probs = sum(w * p for w, p in zip(weights, predictions))
    final_preds = final_probs.argmax(axis=1)
    
    acc = accuracy_score(y_test, final_preds)
    f1 = f1_score(y_test, final_preds, average='macro')
    
    print(f"\nðŸ† TEST ACCURACY: {acc:.4f}")
    print(f"ðŸ† TEST F1 SCORE: {f1:.4f}")
    
    # Confusion matrix
    cm = confusion_matrix(y_test, final_preds)
    print(f"\nðŸ“Š Per-class accuracy:")
    for i, cls in enumerate(le.classes_):
        cls_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0
        print(f"   Class {cls}: {cls_acc:.4f}")
    
    print("\n" + "="*80)
    print("âœ… TRAINING COMPLETE")
    print("="*80)

if __name__ == '__main__':
    main()
